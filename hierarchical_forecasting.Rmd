---
title: 'Hierarchical Time Series Forecasting in the Emergency Departments of NHS Wales'
author: "Janice Hsu"
date: "2023-10-30"
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
                      fig.pos = "H", out.extra = "")
```

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```


\newpage



```{r include = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
# Required Libraries
library(zoo)
library(ggplot2)
library(lubridate)
library(tsibble)
library(tidyverse)
library(fpp3)
library(hts)
library(dplyr)
library(tidyr)
library(forecast)
library(Metrics)
library(purrr)
library(tidyr)
library(stats)
library(fable)
library(dplyr)
library(igraph)
library(ggraph)
library(feasts)

```


```{r include = FALSE, echo = FALSE, message = FALSE, warning = FALSE}
data <- read.csv("HLTH0037_ts_cleaned.csv")
```

# Abstract

## Introduction:

National Health Service (NHS) Wales employs several time series forecasting techniques. In order to support the processes for planning and decision-making, both strategically (at All Wales level) and operationally (at local health board level). However, using base forecasting to analyse individual time series and the aggregated level can cause inconsistencies. Therefore, hierarchical time series forecasting method is to be used in this study. 

## Background:

The COVID-19 outbreak created unexpected difficulties of the forecasting for the attendance of Emergency Department in NHS-Wales. Therefore, NHS-Wales is seeking a reliable and consistent forecasting model regardless of the shifting patterns of patients’ arrivals during the lockdowns and health policy changes. 

## Goal:

This project's main goal is to create a forecasting models for emergency departments that can consistently deliver outcomes at the individual hospital, each local health board, and national levels. The research will allow for a comparison of:

1. Forecasting national time series and with local time series combined.
2. Forecasting time series hierarchically.

By doing this, I hope to create a system that synchronises policymakers' projections with operational forecasts, guaranteeing an efficient decision-making process.


## Techniques:

I will use NHS Wales publicly available data, with an emphasis on emergency departments. The information will be split up into the subsequent sections:

1. National Level Data: This will include all of the information from Wales' emergency departments combined.
2. Local Health Board Data: This will include distinct local health boards which operate individual hospitals.
3. Individual Hospital Data: This will be divided according to distinct hospital.

I'll use the following predicting techniques on the information:

- The national data will be subjected to traditional time series forecasting.
- Summed projections: To get at a nationwide total, individual projections for each hospital and local health boards will be made and then added together.
- Hierarchical Time Series Forecasting: This method models the data in accordance with the underlying hierarchy in the data (national(Wales), local health boards, hospital).


## Differences between base forecasting and hierarchical forecasting:

- Base forecasting: It is used to forecast simple time series data with statistical or machine learning methods. To generate future data, base forecasting observes the historical data, and finds its pattern, trend and seasonality to produce future projections.


- Reconciliation Forecasting (Hierarchical Forecasting): It is used when dealing with hierarchical or grouped time series data. Hierarchical forecasting involves generating forecast on different levels of aggregation. It doesn't only create projections for individual time series, but also reconciles the forecasts to guarantee consistency under multiple hierarchical levels. This method exploits a variety of strategies, such as top down, bottom up, to ensure that the individual projections align with their corresponding aggregated levels. This leads the hierarchical forecasting advantageous due to the fact that it can make sure the consistent and consolidated projections at all levels.


## Anticipated Results:


1. Determine the most reliable and accurate forecasting method for the attendance number for emergency departments in Wales.
2. Provide evidence for why hierarchical time series methods are superior than other forecasting methods.
3. Offer a scalable, transparent method that can be used by other NHS departments.


\newpage

## Data Introduction

The dataset contains variables related to monthly Emergency Attendances in hospitals in Wales, UK. The data was publicly available and retrieved from StatsWales.

- Data: This column represents the aggregated number of attendances in each emergency department. The data was aggregated according to the other columns.

- YearMonth: This column represents data from 2012 April to 2023 June (135 time points).

- Age_Code: This column provides the patient's age group. There are 17 different age groups: "0 to 4","5 to 17","18 to 24","25 to 29","30 to 34","35 to 39","40 to 44","45 to 49","50 to 54","55 to 59","60 to 64","65 to 69","70 to 74","75 to 79","80 to 84","85" and "Unknown".

- Sex_ItemName_ENG: This column provides patient’s gender using 3 categories: “Female”, “Male” and “Unknown”.

- Hospital_Code: This column represents 42 different hospitals in Wales.

- Hospital_ItemName_ENG: This columns refers to the name of the 42 different hospitals in Wales.

- Hospital_Hierarchy: This column represents the code for the health board that the hospital belongs to.

- Hospital_AltCode1: This column provides an alternate code for the hospital.

- Organisation: This column represents the Local Health Board (LHB), which are responsible for planning and delivering NHS Wales services in their areas, including Emergency Care. In Wales there are 7 distinct LHBs, however two of them, Cwm Taf Morgannwg and Swansea Bay, were previously (before 1 st of April 2019) known as Cwm Taf and Abertawe Bro Morgannwg (therefore this column contains 9 distinct values).

- Organisation_Code: A code for the organisation as well as the LHB.

There are three hierarchies in this dataset:
- On the top level, there is all the hospitals in Wales.
- On the second level, there are the different LHB.
- At the bottom level, there are 42 hospitals.



\newpage
# Exploratory Data Analysis

```{r}
# change data structure
data <- data %>%
  mutate(YearMonth = yearmonth(YearMonth)) %>%
  as_tsibble(index = YearMonth, key = c(Age_Code, Sex_ItemName_ENG, Hospital_Code, Hospital_ItemName_ENG)) 

```



## Number of patients entering ED under different hospital hierarchy
```{r}
# Aggregate the data
data_hts <- data %>%
  aggregate_key(Organisation/Hospital_ItemName_ENG, attendance = sum(Data))

# Plot the aggregated data

data_hts |>
  filter(is_aggregated(Hospital_ItemName_ENG)) |>
  autoplot(attendance) +
  labs(y = "Number of patients",
       title = "Number of patients who enter ED") +
  facet_wrap(vars(Organisation), scales = "free_y", ncol = 3) +
  theme(legend.position = "none")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 0, hjust = 1)) 
```

As already mentioned, two LHBs were redefined from the 1st of April 2019 onwards: Cwm Taf ((# hospitals = 27)--> Cwm Taf Morgannwg ((# hospitals = 30)// Abertawe Bro Morgannwg ((# hospitals = 26) --> Swansea Bay ((# hospitals = 31). 

Since the number of hospitals included before and after the change in each LHB was different, these 4 organisations were aggregated into one unique group to enable their inclusion in the forecasting analysis (defined as “Grouped_4_organisation”in the code snippets).

\newpage

## Group the changed Local Health Board together

```{r}
# mutate Aggregated_Organisation due to the change of the health boards

data_grouped <- data %>%
  mutate(Aggregated_Organisation = case_when(
    Organisation %in% c("Cwm Taf", "Cwm Taf Morgannwg", "Abertawe Bro Morgannwg", "Swansea Bay") ~ "Grouped_4_organisation",
    TRUE ~ Organisation
  ))

```

### There are 6 Local Health Boards
```{r}
unique(data_grouped$Aggregated_Organisation)
```

**What are Local Health Board?**
The Welsh Government oversees the country's healthcare system, which is different from that of the rest of the UK. In Wales, an institution designated as a "Local Health Board" (LHB) is in charge of providing all NHS healthcare services within a particular geographic region.

Planning and executing NHS services in their respective regions is the responsibility of local health boards. These medical services consist of:

- pharmacy
- dental
- optical, and mental health

They also bear accountability for:

- enhancing the results of physical and mental health
- promoting well being
- decreasing disparities in population health
- contracting outside organizations to provide services in order to address residents' needs

(NHS Wales Health Boards and Trusts | GOV.WALES, 2023)

Ref: NHS Wales health boards and trusts | GOV.WALES. (2023, February 3). GOV.WALES. https://www.gov.wales/nhs-wales-health-boards-and-trusts


```{r}
data2_hts <- data_grouped %>%
  group_by(Aggregated_Organisation) %>%
  summarise(attendance = sum(Data)) 
```




## Number of patients who enter ED under 6 different local health boards
```{r}
data2_hts |>
  ggplot(aes(x = YearMonth, y = attendance)) +
  geom_line(stat = "identity") +
  labs(y = "Number of patients",
       title = "Number of patients who enter ED") +
  facet_wrap(vars(Aggregated_Organisation), scales = "free_y", ncol = 3) +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


**Analysing Health Board Attendance Trends Amidst and Post-COVID-19:**

Healthcare systems around the world faced significant difficulties during the COVID-19 epidemic, which caused them to quickly shift into crisis-response mode. Intriguing trends during and after the epidemic era have been revealed by a thorough review of attendance data from local health boards.

- During the pandemic:

Surprisingly, several local health boards experienced a large reduction in attendance throughout the epidemic period. This significant drop in attendance, which may have been brought on by reasons including stringent public health regulations, widespread concern over virus exposure, and maybe changes in healthcare delivery strategies, was an anomalous departure from historical attendance trends.

- Post pandemic:

After the pandemic's intensity started to lessen, there was a noticeable increase in attendance, however there were notable differences amongst the local health boards. In contrast to most boards, which showed a strong recovery and raised their attendance figures to levels prior to the pandemic, Powys Teaching stood out as an outlier and defied this recovery trend. To fully understand the subtleties affecting these disparate courses and influence upcoming strategic planning, more research is necessary.

- Findings on seasonality:

Initial data analysis reveals the existence of seasonality, which is characterized by repeated swings in attendance across all health boards. This pattern calls for a perceptive investigation into:

Identify Underlying Causes: 
Researching seasonality's probable triggers, such as public health initiatives, flu seasons, or holiday seasons, in order to comprehend the factors influencing these patterns.

Determine Seasonal influence: 
Establishing whether the observed seasonality has an equal influence on all health boards or whether there are differences, which may be a sign of regional causes or policy changes.

Planning and Forecasting: 
By using this understood seasonality to improve forecasting models and ensuring that they are calibrated to account for these seasonal changes, more precise and useful predictive insights are made possible.


\newpage

## Seasonality of number of attendances

To further explore whether there was any trend and/or seasonality in the data by LHB, the time series was decomposed using the STL method and the extracted components (trend, seasonal and residuals) plotted:

```{r}
data_grouped |> 
  group_by(Aggregated_Organisation) |>
  summarise(`Number of patients` = sum(Data)) |>
  gg_season() +
  facet_wrap(vars(Aggregated_Organisation), scales = "free_y", nrow = 3)+
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Seasonal plots of ED Attendances (by LHB)")
```
- Findings:

1. Most LHBs have a trend whereby attendance is lower in some months, like April and November, and higher in others, like June and July.

2. In many of the LHBs, the attendance in 2020 differs noticeably from other years.
In comparison to similar months in past years, the majority of LHBs appear to have a fall or reduction in ED attendances during several months of 2020. This could be a sign of outside influences influencing access to or use of healthcare because of the pandemic.
3. 
- Aneurin Bevan: The middle of the year for 2020 shows a clear decline.
- Cardiff & Vale: Attendances in 2020 appear to be continuously lower than in previous years, particularly in the early months.
- Hywel Dda: Around mid-year, there is a noticeable decline in attendance.
- Betsi Cadwaladr: While the trend for 2020 appears to be largely constant, several months still show a decline in attendance.
- Grouped_4_organisation: Attendance clearly drops off during particular months.
- Powys Teaching: The variations are more pronounced, with obvious dips during particular months.



### Decompose Time Series

```{r}

# Decompose time series for each health board using STL
stl_decompositions <- data2_hts %>%
  split(.$Aggregated_Organisation) %>%
  purrr::map(function(data){
    ts_data <- ts(data$attendance, frequency = 12)
    stl(ts_data, s.window = "periodic")
  })

# Convert the decompositions to a tidy data frame
stl_df <- purrr::map2_dfr(stl_decompositions, names(stl_decompositions), 
                          ~{
                            time_series <- as.data.frame(.x$time.series)
                            time_series$Aggregated_Organisation <- .y
                            return(time_series)
                          })

```


```{r}
num_months <- length(unique(data2_hts$YearMonth))
num_orgs <- length(unique(data2_hts$Aggregated_Organisation))

# Correct assignment for YearMonth column
stl_df$YearMonth <- rep(unique(data2_hts$YearMonth), times = num_orgs)

```

### Plotting

```{r}
stl_df_tidy <- stl_df %>%
  tidyr::pivot_longer(cols = c(trend, seasonal, remainder), 
                      names_to = "component", 
                      values_to = "value")

```


```{r}
stl_df_tidy %>%
  ggplot(aes(x = YearMonth, y = value, color = component)) +
  geom_line() +
  facet_wrap(vars(Aggregated_Organisation), scales = "free_y", ncol = 3) +
  labs(title = "STL Decomposition of ED Attendance",
       y = "Number of Patients",
       color = "Component") +
  scale_color_manual(values = c("blue", "green", "red"),
                     breaks = c("trend", "seasonal", "remainder"),
                     labels = c("Trend", "Seasonal", "Residual")) +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1))

```

- Findings:

**1. Trend:**
There appears to be an upward trend in the number of ED visits over time for all LHBs, but there are outliers.
The most obvious decline occurs around 2020, which may be a sign of the COVID-19 pandemic's effects.

**2. Seasonality:**
The seasonal component illustrates the periodicity in ED attendances by showing predictable patterns that repeat annually.
There is a pronounced surge in the number of patient's attendance in the middle of the year (approximately in June or July). This seasonal pattern underlines the recurrent nature of patient admissions.

**3. Residuals:**
The residuals are the data's unexplained variation after the trend and seasonal components have been removed.
The majority of LHBs have residuals that oscillate near zero, which shows that the STL decomposition has effectively caught the main trends in the data.





\newpage

## Change the Age_Code structure into different groups for simplicity and interpretability

```{r}
unique(data_grouped$Age_Code)
```


### Age group: "0-4", "5-17", "18-69", "70^"
```{r}
data_grouped_age <- data_grouped %>%
  filter(Age_Code != "Unknown") %>%
  mutate(Grouped_Age = case_when(
    Age_Code == "0 to 4" ~ "0-4",
    Age_Code == "5 to 17" ~ "5-17",
    Age_Code %in% c("18 to 24", "25 to 29", "30 to 34", "35 to 39", 
                    "40 to 44", "45 to 49", "50 to 54", "55 to 59", 
                    "60 to 64", "65 to 69") ~ "18-69",
    Age_Code %in% c("70 to 74", "75 to 79", "80 to 84", "85") ~ "70 and over",
    TRUE ~ "Other"
  ))

```


## Plot Number of Patients in different age groups
```{r}
data_gts <- data_grouped_age %>%
  filter(Sex_ItemName_ENG != "Not Specified or invalid") %>%
  group_by(Grouped_Age, Sex_ItemName_ENG) %>%
  summarize(Number = sum(Data, na.rm = TRUE))

ggplot(data_gts, aes(x = Grouped_Age, y = Number)) +
  geom_bar(stat = "identity") +
  labs(y = "Number of patients") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~ Sex_ItemName_ENG, scales = "free")


```


- Findings:

It is expected that the age group 18-69 has the most amount of attendance, given the fact that it is the biggest group among all. However, the second largest group is from the oldest age bracket. It is aligned with the general understanding of that elders may need more health care. 

```{r}
#Change the data into wide format
data2_wide <- data_grouped %>%
  group_by(Aggregated_Organisation) %>%
  index_by(YearMonth) %>%
  summarise(attendance = sum(Data)) %>%
  pivot_wider(names_from = Aggregated_Organisation, values_from = attendance)


```

```{r}
data2_wide <- as_tibble(data2_wide)
data2_wide <- data2_wide %>%
   mutate(Total = rowSums(select(., c("Aneurin Bevan", "Betsi Cadwaladr", "Cardiff & Vale", "Grouped_4_organisation", "Hywel Dda", "Powys Teaching")), na.rm = TRUE))

```



# Forecast with ARIMA

AutoRegressive Integrated Moving Average, or ARIMA, is a widely used forecasting technique for time series data. 

- AutoRegressive, or AR:

The link between one observation and several lag observations (prior periods or terms) is the subject of this component.
The word "auto" denotes that the variable is being regressed against itself.

- I (Integrated):

This is the original data difference that is used to make the time series stationary, meaning that the data values are independent of time.
The mean of stationary data will not change over time, and neither will the variance. It won't display seasonality or patterns.

- Moving Average, or MA:

This part uses a combination of earlier error terms to model the time series error.
The MA term's intuitive purpose is to model the error.
 


```{r}

# Define the forecast horizon and validation period
h <- 12
validation_period <- 6

```



```{r}
# Splitting the data into training and testing sets
training_data <- head(data2_wide, nrow(data2_wide) - validation_period)
test_data <- tail(data2_wide, validation_period)

```


```{r}

cols_to_forecast <- c("Aneurin Bevan", "Betsi Cadwaladr", "Cardiff & Vale", "Grouped_4_organisation", "Hywel Dda", "Powys Teaching", "Total")
forecast_list <- list()

arima_forecast_list <- list()
start_year <- year(min(training_data$YearMonth))
start_month <- month(min(training_data$YearMonth))

for (col in cols_to_forecast) {
  ts_data <- ts(training_data[[col]], start = c(start_year, start_month), frequency = 12)
  arima_model <- auto.arima(ts_data)
  forecast_list[[col]] <- forecast(arima_model, h = h)
}
```

## Plotting using ARIMA

```{r  fig.height=2, fig.width=3}

for (col in cols_to_forecast) {
  end_year_train <- year(max(training_data$YearMonth))
  end_month_train <- month(max(training_data$YearMonth))
  
  start_year_test <- ifelse(end_month_train == 12, end_year_train + 1, end_year_train)
  start_month_test <- ifelse(end_month_train == 12, 1, end_month_train + 1)
  
  actual_ts <- ts(test_data[[col]], start = c(start_year_test, start_month_test), frequency = 12)
  
  plot_forecast <- autoplot(forecast_list[[col]]) +
    autolayer(actual_ts, series="Actual", PI=FALSE) +
    labs(title = paste(col)) +
    ylab("attendance") + 
    theme(legend.position = "bottom")
  
  print(plot_forecast)
}
```

# Forecasting with ETS

Error, Trend, Seasonality, or ETS, is a widely used model for predicting time series data. A generalization of these techniques, the ETS model includes a variety of exponential smoothing methods.

The elements of the ETS model breaks down as follows:


**Error (E):**

The error component represents the discrepancies between observed values and those predicted by a model. There are two primary types of error models: additive and multiplicative.

1. **Additive Error**: 
   - Assumes that the model's residuals (errors) are added together.
   - Suitable when the magnitude of errors remains relatively constant regardless of the time series' level.

2. **Multiplicative Error**: 
   - Assumes that the errors change proportionally to the time series' level.
   - Relevant when the magnitude of errors increases or decreases in proportion to the actual value of the series.

Certainly! Here's a refined description of the Trend component:



**Trend (T):**

The trend component captures the long-term progression of a time series.

1. **No Trend**: 
   - Indicates that the time series remains relatively constant over time and does not exhibit any discernible upward or downward movement.

2. **Additive Trend**: 
   - Represents a consistent linear progression in the time series. 
   - The series displays a steady increase or decrease at a constant amount over equal intervals of time.

3. **Multiplicative Trend**: 
   - Denotes an exponential progression in the series. 
   - The data exhibits growth or decline at a rate that increases or decreases proportionally over time.


**Seasonality (S):**

The seasonality component captures periodic fluctuations in a time series that recur at regular intervals, such as daily, monthly, or yearly.

1. **No Seasonality**: 
   - Implies that the time series does not exhibit any periodic or recurring fluctuations. The data remains consistent throughout without any noticeable seasonal patterns.

2. **Additive Seasonality**: 
   - Suggests that seasonal patterns are consistent in magnitude throughout the time series. 
   - The seasonal effect is added to or subtracted from the series in a constant manner, irrespective of the time series level.

3. **Multiplicative Seasonality**: 
   - Indicates that the seasonal fluctuations change in proportion to the time series' level. 
   - As the series grows or declines, the impact of seasonality amplifies or diminishes respectively, leading to varying magnitudes of seasonal effects over time.








```{r}

ets_forecast_list <- list()
start_year <- year(min(training_data$YearMonth))
start_month <- month(min(training_data$YearMonth))

for (col in cols_to_forecast) {
  ts_data <- ts(training_data[[col]], start = c(start_year, start_month), frequency = 12)
  ets_model <- ets(ts_data)  # Use the ets function
  forecast_list[[col]] <- forecast(ets_model, h = h)
}


```

## Plotting with ets
```{r  fig.height=2, fig.width=3}
for (col in cols_to_forecast) {
  end_year_train <- year(max(training_data$YearMonth))
  end_month_train <- month(max(training_data$YearMonth))
  
  start_year_test <- ifelse(end_month_train == 12, end_year_train + 1, end_year_train)
  start_month_test <- ifelse(end_month_train == 12, 1, end_month_train + 1)
  
  actual_ts <- ts(test_data[[col]], start = c(start_year_test, start_month_test), frequency = 12)
  
  plot_forecast <- autoplot(forecast_list[[col]]) +
    autolayer(actual_ts, series="Actual", PI=FALSE) +
    labs(title = paste(col)) +
    ylab("attendance") + 
    theme(legend.position = "bottom")
  
  print(plot_forecast)
}

```


**Conformity:**

Hywel Dda, Powys Teaching, Cardiff & Vale, Grouped_4_Organisation, and Total (All-Wales): For these health boards, the ARIMA model effectively predicted the downturn that was observed in the latter portions of 2023 and successfully reflected the actual data, indicating a cogent direction. This demonstrates ARIMA's ability to effectively model and anticipate time-series data for these specific entities, despite the unpredictably fluctuating conditions brought on by the epidemic.

**Contradictions:**

Aneurin Bevan and Betsi Cadwaladr: By contrast, the prediction statistics for Aneurin Bevan and Betsi Cadwaladr show a clear departure from the actual numbers. It is necessary to assess the ARIMA model's applicability and accuracy with regard to these specific datasets because there is a discrepancy between the predicted value and the real-time data.


**Comparison:**

Upon first glance, it appears that the ETS model offers a relatively close alignment with the real data, demonstrating a strong predictive power. When compared to the ARIMA model, the projections closely mimic the actual data patterns rather than merely following them, which suggests a possibility for greater accuracy.

**Findings: Consistent Decline in Predictions**

Downtrend in 2023: Regardless of the model used, all forecasts uniformly point to a decline in trends during the second half of 2023. This might be explained by the historical data's apparent cyclical nature. In other words, the historical data reveals a pattern in which peaks frequently appear in the middle of the year and then descend into troughs at year's end and the beginning of the next year. This recurrent pattern may have a significant impact on the expected downturn in 2023.

In light of the above, it is crucial to approach model selection with a nuanced comprehension of the underlying data and impacting factors, even though the ETS model shows a potentially higher prediction accuracy within the given circumstances. 

An accuracy analysis was then carried out upon for both ARIMA and ETS models to further assess the quality of their predictions.

# Accuracy assessment for ARIMA and ETS


```{r}
# Lists to store forecasts and error metrics
arima_forecast_list <- list()
ets_forecast_list <- list()
arima_error_metrics <- list()
ets_error_metrics <- list()
```


```{r}
# Forecasting with ARIMA and ETS
for (col in cols_to_forecast) {
  ts_data <- ts(training_data[[col]], start = c(start_year, start_month), frequency = 12)
  
  # ARIMA model
  arima_model <- auto.arima(ts_data)
  arima_forecast <- forecast(arima_model, h = h)
  arima_forecast_list[[col]] <- arima_forecast
  
  # ETS model
  tryCatch({
    ets_model <- ets(ts_data)
    ets_forecast <- forecast(ets_model, h = h)
    ets_forecast_list[[col]] <- ets_forecast
  }, error = function(e) {
    cat("Error in forecasting for column:", col, "with message:", e$message, "\n")
  })
}
```

## Calculating Accuracy Metrics for ARIMA and ETS
```{r}

for (col in cols_to_forecast) {
  actual <- test_data[[col]]
  
  arima_forecasted <- head(arima_forecast_list[[col]]$mean, validation_period)
  
  # ARIMA metrics
  arima_MAE <- mae(actual, arima_forecasted)
  arima_RMSE <- rmse(actual, arima_forecasted)
  arima_MAPE <- tryCatch(mape(actual, arima_forecasted), error = function(e) NA)
  arima_error_metrics[[col]] <- list(MAE = arima_MAE, RMSE = arima_RMSE, MAPE = arima_MAPE)
  
  # Check if ETS forecast exists for the column to avoid errors
  if (!is.null(ets_forecast_list[[col]])) {
    ets_forecasted <- head(ets_forecast_list[[col]]$mean, validation_period)
    
    # ETS metrics
    ets_MAE <- mae(actual, ets_forecasted)
    ets_RMSE <- rmse(actual, ets_forecasted)
    ets_MAPE <- tryCatch(mape(actual, ets_forecasted), error = function(e) NA)
    ets_error_metrics[[col]] <- list(MAE = ets_MAE, RMSE = ets_RMSE, MAPE = ets_MAPE)
  }
}

```

### Displaying Accuracy Metrics for each Column
```{r}
TableAccuracyMetrics <-
  rbind(
    # aggregate ARIMA accuracy metrics
    as.data.frame(
      do.call(rbind,unlist(arima_error_metrics, recursive=FALSE))
      ) |>
      rename(Metric = V1) |>
      rownames_to_column("LHB") |>
      mutate(
        Metric = round(Metric,3),
        Model = "ARIMA"
        ),
    
    # aggregate ETS accuracy metrics
    as.data.frame(
      do.call(rbind,unlist(ets_error_metrics, recursive=FALSE))
      ) |>
      rename(Metric = V1) |>
      rownames_to_column("LHB") |>
      mutate(
        Metric = round(Metric,3),
        Model = "ETS"
        )
    ) |>
  
  ## wrangle table from long to wide format
  tidyr::pivot_wider(
    names_from = Model, 
    values_from = Metric
  )

TableAccuracyMetrics |> knitr::kable()
```



- With Cardiff & Vale being a significant exception, most places displayed reduced error metrics when modeled with ETS, signifying a higher predictive accuracy as compared to the ARIMA model.


- According to the interest expressed by the NHS executives, I would like to provide some insights for the error metrics according to the quality of the forecast for the non-zero count data:


1. **Root Mean Squared Error (RMSE)**
   - **Description**: Measures the square root of the average squared differences between forecast and actual values.
   - **Pros**: It gives more weight to large errors.
   - **Cons**: It can be influenced significantly by outliers. RMSE might be inflated, if the count data is prone to significant spikes or declines, which should not be the problem for this data except for the Covid-19 era.
   
2. **Mean Absolute Percentage Error (MAPE)**
   - **Description**: Measures the average of the absolute percentage errors.
   - **Pros**: It is a relative metric, and is scale-independent and easy to interpret.
   - **Cons**: MAPE can overemphasize relative errors on small counts.
   
3. **Mean Absolute Error (MAE)**
   - **Description**: Measures the average of the absolute differences between forecast and actual values.
   - **Pros**: It's less sensitive to outliers than RMSE. It provides a straightforward average error size.
   - **Cons**: It does not emphasize large errors.

**However, RMSE and MAE should not be used for the hierarchical time series data.**
And here is why:

- **1. Consistency Between Levels:**
Granularity Errors are not consistently distributed or similar across all hierarchical levels due to the variance in data granularity between levels, and there will be an aggregation dilemma. Due to the variation in magnitude and volume at various hierarchical levels, direct aggregation of RMSE or MAE from bottom levels to top levels might produce inconsistent or biased judgments of forecast accuracy.

- **2. Problems with reconciliation:**
It is possible for forecasts to not add up coherently in a hierarchical structure when they are independently generated for several levels. Since they assess mistakes without taking into account hierarchical dependencies, RMSE and MAE do not by default address this reconciliation. And the alignment issue will therefore be caused. The use of standard error measurements at various hierarchical levels may produce false conclusions regarding the precision and dependability of the forecasts in the absence of a cogent reconciliation process.

- **3. Metric incomparability:**
Impact of a unit error is not constant across levels; for instance, a 100-unit error may be insignificant at a higher aggregate level but significant at a lower one. Additionally, due to the divergent scales of the errors at various levels, it is difficult to use a single statistic, such as RMSE or MAE, to fairly compare performance across all hierarchical levels.




# Reconciliation



## Aggregate data

```{r}
# hierarchy with age group
data_gts <- data_grouped_age |>
  aggregate_key((Aggregated_Organisation / Hospital_ItemName_ENG) * Grouped_Age, attendance = sum(Data))
```

- 

```{r}
data_gts <- data_gts %>%
  as_tsibble(key = c("Aggregated_Organisation", "Grouped_Age", "Hospital_ItemName_ENG"), index = "YearMonth")


```


```{r}
library(dplyr)

# Define the expected number of rows for the entire time series
expected_length <- nrow(unique(data_gts[,"YearMonth"]))
# Obtain the data for the entire time series for the purpose of reconciliation
cleaned_data <- data_gts %>%
  group_by(Aggregated_Organisation, Grouped_Age, Hospital_ItemName_ENG) %>%
  filter(n() == expected_length) %>%
  ungroup()

```


## Time series cross validation
## stretch_tsibble(.init = 135-12, .step = 1)

```{r}
library(fable.tscount)
library(tscount)
# Split data
training_data <- cleaned_data %>% 
  filter(YearMonth <= yearmonth("2022 Jun"))

test_data <- cleaned_data %>% 
  filter(YearMonth > yearmonth("2022 Jun")) 

test_data_filtered <- test_data %>%
  filter(Aggregated_Organisation == "<aggregated>")

```

```{r}
library(tsibble)

start_date <- min(training_data$YearMonth)
end_date <- max(training_data$YearMonth)

# Calculate the difference in months using yearmonth
number_of_months <- as.integer(yearmonth(end_date) - yearmonth(start_date)) + 1

number_of_months

```

135-12
```{r}
data_gts_tr <- cleaned_data |>
  stretch_tsibble(.init = 123, .step = 1) 


```


```{r}
filtered_tr <- data_gts_tr %>%
  filter(Aggregated_Organisation == "<aggregated>")
```


```{r}
data_gts_fc <-filtered_tr |>
  model(
    naive_model = NAIVE(attendance),
    ets_model = ETS(attendance),
    tscount = TSCOUNT(attendance ~ trend() + season() , link = "log", model = list(past_obs = 1:12))
  )|> mutate (comb = (naive_model+ets_model+tscount)/3) %>%
  forecast(h = 12) 
```




```{r}


merged_data <- left_join(as_tibble(data_gts_fc), as_tibble(test_data_filtered), by = c("YearMonth", "Aggregated_Organisation","Grouped_Age", "Hospital_ItemName_ENG"))

# Create an empty list to store results
results_list <- list()

# List of models
model_names <- unique(merged_data$.model)

# List of training datasets (using .id)
training_ids <- unique(merged_data$.id)

# Nested loop: first loop through models, then loop through training datasets
for (model in model_names) {
  results_list[[model]] <- list()  # Initialize a nested list for each model
  
  for (id in training_ids) {
    # Filter the data for the current model and training dataset
    subset_data <- merged_data %>% filter(.model == model & .id == id)
    
    # Calculate MASE and RMSSE
    mase_val <- mean(abs((subset_data$attendance.y - subset_data$.mean) / 
                         (subset_data$attendance.y - lag(subset_data$attendance.y))), na.rm = TRUE)
    rmsse_val <- sqrt(mean((subset_data$attendance.y - subset_data$.mean)^2, na.rm = TRUE))
    
    # Store results in the nested list
    results_list[[model]][[as.character(id)]] <- list(MASE = mase_val, RMSSE = rmsse_val)
  }
}

# List to store averaged results
averaged_results <- list()

for (model in model_names) {
  # Extract the list of results for the current model
  model_results <- results_list[[model]]
  
  # Calculate the average MASE and RMSSE over the 13 training datasets while removing NA values
  avg_mase <- mean(sapply(model_results, function(x) x$MASE), na.rm = TRUE)
  avg_rmsse <- mean(sapply(model_results, function(x) x$RMSSE), na.rm = TRUE)
  
  # Store the averaged results
  averaged_results[[model]] <- list(Avg_MASE = avg_mase, Avg_RMSSE = avg_rmsse)
}


# Convert averaged_results to a dataframe
averaged_df <- data.frame(model = names(averaged_results))

# Extract Avg_MASE and Avg_RMSSE values for each model
averaged_df$Avg_MASE <- sapply(averaged_results, function(x) x$Avg_MASE)
averaged_df$Avg_RMSSE <- sapply(averaged_results, function(x) x$Avg_RMSSE)

print(averaged_df)



```



### Time series cross validation, set training data before Covid
```{r}
library(fable.tscount)
library(tscount)
# Split data
training_data2 <- cleaned_data %>% 
  filter(YearMonth <= yearmonth("2021 Jan"))

test_data2 <- cleaned_data %>% 
  filter(YearMonth > yearmonth("2021 Jan")) 

test_data_filtered2 <- test_data2 %>%
  filter(Aggregated_Organisation == "<aggregated>")

```

```{r}
library(tsibble)

start_date <- min(training_data2$YearMonth)
end_date <- max(training_data2$YearMonth)

# Calculate the difference in months using yearmonth
number_of_months <- as.integer(yearmonth(end_date) - yearmonth(start_date)) + 1

number_of_months

```

135-41
```{r}
data_gts_tr2 <- training_data2 |>
  stretch_tsibble(.init = 94, .step = 1) 


```


```{r}
filtered_tr2 <- data_gts_tr2 %>%
  filter(Aggregated_Organisation == "<aggregated>")
```


```{r}
data_gts_fc2 <-filtered_tr2 |>
  model(
    naive_model = NAIVE(attendance),
    ets_model = ETS(attendance),
    tscount = TSCOUNT(attendance ~ trend() + season() , link = "log", model = list(past_obs = 1:12))
  )|> mutate (comb = (naive_model+ets_model+tscount)/3) %>%
  forecast(h = 29) 
```




```{r}


merged_data2 <- left_join(as_tibble(data_gts_fc2), as_tibble(test_data_filtered2), by = c("YearMonth", "Aggregated_Organisation","Grouped_Age", "Hospital_ItemName_ENG"))

# Create an empty list to store results
results_list <- list()

# List of models
model_names <- unique(merged_data2$.model)

# List of training datasets (using .id)
training_ids <- unique(merged_data2$.id)

# Nested loop: first loop through models, then loop through training datasets
for (model in model_names) {
  results_list[[model]] <- list()  # Initialize a nested list for each model
  
  for (id in training_ids) {
    # Filter the data for the current model and training dataset
    subset_data <- merged_data2 %>% filter(.model == model & .id == id)
    
    # Calculate MASE and RMSSE
    mase_val <- mean(abs((subset_data$attendance.y - subset_data$.mean) / 
                         (subset_data$attendance.y - lag(subset_data$attendance.y))), na.rm = TRUE)
    rmsse_val <- sqrt(mean((subset_data$attendance.y - subset_data$.mean)^2, na.rm = TRUE))
    
    # Store results in the nested list
    results_list[[model]][[as.character(id)]] <- list(MASE = mase_val, RMSSE = rmsse_val)
  }
}

# List to store averaged results
averaged_results <- list()

for (model in model_names) {
  # Extract the list of results for the current model
  model_results <- results_list[[model]]
  
  # Calculate the average MASE and RMSSE over the 13 training datasets while removing NA values
  avg_mase <- mean(sapply(model_results, function(x) x$MASE), na.rm = TRUE)
  avg_rmsse <- mean(sapply(model_results, function(x) x$RMSSE), na.rm = TRUE)
  
  # Store the averaged results
  averaged_results[[model]] <- list(Avg_MASE = avg_mase, Avg_RMSSE = avg_rmsse)
}


# Convert averaged_results to a dataframe
averaged_df2 <- data.frame(model = names(averaged_results))

# Extract Avg_MASE and Avg_RMSSE values for each model
averaged_df2$Avg_MASE <- sapply(averaged_results, function(x) x$Avg_MASE)
averaged_df2$Avg_RMSSE <- sapply(averaged_results, function(x) x$Avg_RMSSE)

print(averaged_df2)



```

## Forecast

```{r}
# remove naive_model, and use comb as the base model
library(fable.tscount)
fit_gts <- cleaned_data %>%
 model(
    ets_model = ETS(attendance),
    tscount = TSCOUNT(attendance ~ trend() + season() , link = "log", model = list(past_obs = 1:12))
  )|> mutate (comb = (ets_model+tscount)/2)  %>%
  reconcile(
    bu = bottom_up(comb),
    ols = min_trace(comb, method = "ols")
  )
```

```{r}
fc_gts <- fit_gts %>%
  forecast(h = 6)

```

```{r}
fc_gts |>
  filter(is_aggregated(Hospital_ItemName_ENG), is_aggregated(Grouped_Age)) |>
  autoplot(
    cleaned_data,
    level = NULL
  ) +
  labs(y = "attendance") +
  facet_wrap(vars(Aggregated_Organisation), scales = "free_y")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
```{r}
fc_gts |>
  filter(is_aggregated(Hospital_ItemName_ENG), is_aggregated(Grouped_Age)) |>
  autoplot(
    cleaned_data %>% filter(YearMonth >= yearmonth("2022 Jan")),
    level = NULL
  ) +
  labs(y = "attendance") +
  facet_wrap(vars(Aggregated_Organisation), scales = "free_y")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


















```{r}
library(igraph)

# Nodes data
nodes_df <- data.frame(
  name = c("All Wales", 
           "Betsi Cadwaladr", "Hywel Dda", "Grouped_4_organisation", 
           "Cardiff & Vale", "Aneurin Bevan", "Powys Teaching", 
           "Ysbyty Glan Clwyd", "Wrexham Maelor Hospital", "Colwyn Bay Community Hospital", 
           "Holywell Community Hospital", "Mold Community Hospital"),
  stringsAsFactors = FALSE
)

# Edges data
edges_df <- data.frame(
  from = c(rep("All Wales", 6), 
           rep("Betsi Cadwaladr", 5)),
  to = c("Betsi Cadwaladr", "Hywel Dda", "Grouped_4_organisation", 
         "Cardiff & Vale", "Aneurin Bevan", "Powys Teaching",
         "Ysbyty Glan Clwyd", "Wrexham Maelor Hospital", "Colwyn Bay Community Hospital", 
         "Holywell Community Hospital", "Mold Community Hospital"),
  stringsAsFactors = FALSE
)

# Creating graph object
graph <- graph_from_data_frame(d = edges_df, vertices = nodes_df, directed = TRUE)

# Color vector: 1 color for "All Wales", 6 colors for organisations, and 5 colors for hospitals
vertex_colors <- c("skyblue", rep("lightgreen", 6), rep("lightpink", 5))

# Plot the graph
plot(graph, 
     vertex.size = 15, 
     vertex.label.cex = 0.6,  
     vertex.color = vertex_colors,  
     edge.arrow.size = 0.5, 
     vertex.label.degree = pi/4, # 45-degree label rotation
     layout = layout.reingold.tilford(graph, root = 1))

```

## Comment:
1. use another model such as probabilistic forecasting
2. multivariate model, try to get global variable instead of only local variables
3. separate the training dataset before covid and after covid and see if the accuracy changes

