---
title: "hierarchical_forecasting_report"
author: "Janice Hsu"
date: "2023-09-30"
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
---
- Change the accuracy test (the one with 0)
- probably remove the seasonal effect and see if there's a pattern or sth
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
                      fig.pos = "H", out.extra = "")
```

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```


\newpage

# Preliminary Analysis

```{r}
# Required Libraries
library(zoo)
library(ggplot2)
library(lubridate)
library(tsibble)
library(tidyverse)
library(fpp3)
library(hts)
library(dplyr)
library(tidyr)
library(forecast)
library(Metrics)
library(purrr)
library(tidyr)
library(stats)
library(fable)
library(dplyr)
library(igraph)
library(ggraph)
library(feasts)

```


```{r}
data <- read.csv("HLTH0037_ts_cleaned.csv")
```


## Data Introduction

The dataset contains variables related to monthly Emergency Attendances in hospitals in Wales, UK. The data was publicly available and retrieved from StatsWales.

- Data: This column represents the aggregated number of attendances in each emergency department. The data was aggregated according to the other columns.

- YearMonth: This column represents data from 2012 April to 2023 June (135 time points).

- Age_Code: This column provides the patient's age group. There are 17 different age groups: "0 to 4","5 to 17","18 to 24","25 to 29","30 to 34","35 to 39","40 to 44","45 to 49","50 to 54","55 to 59","60 to 64","65 to 69","70 to 74","75 to 79","80 to 84","85" and "Unknown".

- Sex_ItemName_ENG: This column provides patient’s gender using 3 categories: “Female”, “Male” and “Unknown”.

- Hospital_Code: This column represents 42 different hospitals in Wales.

- Hospital_ItemName_ENG: This columns refers to the name of the 42 different hospitals in Wales.

- Hospital_Hierarchy: This column represents the code for the health board that the hospital belongs to.

- Hospital_AltCode1: This column provides an alternate code for the hospital.

- Organisation: This column represents the Local Health Board (LHB), which are responsible for planning and delivering NHS Wales services in their areas, including Emergency Care. In Wales there are 7 distinct LHBs, however two of them, Cwm Taf Morgannwg and Swansea Bay, were previously (before 1 st of April 2019) known as Cwm Taf and Abertawe Bro Morgannwg (therefore this column contains 9 distinct values).

- Organisation_Code: A code for the organisation as well as the LHB.

There are three hierarchies in this dataset:
- On the top level, there is all the hospitals in Wales.
- On the second level, there are the different LHB.
- At the bottom level, there are 42 hospitals.



\newpage
# Exploratory Data Analysis

```{r}
# change data structure
data <- data %>%
  mutate(YearMonth = yearmonth(YearMonth)) %>%
  as_tsibble(index = YearMonth, key = c(Age_Code, Sex_ItemName_ENG, Hospital_Code, Hospital_ItemName_ENG)) 

```



## Number of patients entering ED under different hospital hierarchy
```{r}
# Aggregate the data
data_hts <- data %>%
  aggregate_key(Organisation/Hospital_ItemName_ENG, attendance = sum(Data))

# Plot the aggregated data

data_hts |>
  filter(is_aggregated(Hospital_ItemName_ENG)) |>
  autoplot(attendance) +
  labs(y = "Number of patients",
       title = "Number of patients who enter ED") +
  facet_wrap(vars(Organisation), scales = "free_y", ncol = 3) +
  theme(legend.position = "none")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 0, hjust = 1)) 
```

As already mentioned, two LHBs were redefined from the 1st of April 2019 onwards: Cwm Taf ((# hospitals = 27)--> Cwm Taf Morgannwg ((# hospitals = 30)// Abertawe Bro Morgannwg ((# hospitals = 26) --> Swansea Bay ((# hospitals = 31). 

Since the number of hospitals included before and after the change in each LHB was different, these 4 organisations were aggregated into one unique group to enable their inclusion in the forecasting analysis (defined as “Grouped_4_organisation”in the code snippets).

\newpage

## Group the changed Local Health Board together

```{r}
# mutate Aggregated_Organisation due to the change of the health boards

data_grouped <- data %>%
  mutate(Aggregated_Organisation = case_when(
    Organisation %in% c("Cwm Taf", "Cwm Taf Morgannwg", "Abertawe Bro Morgannwg", "Swansea Bay") ~ "Grouped_4_organisation",
    TRUE ~ Organisation
  ))

```

### There are 6 Local Health Boards
```{r}
unique(data_grouped$Aggregated_Organisation)
```

```{r}
data2_hts <- data_grouped %>%
  group_by(Aggregated_Organisation) %>%
  summarise(attendance = sum(Data)) 
```




## Number of patients who enter ED under 6 different local health boards
```{r}
data2_hts |>
  ggplot(aes(x = YearMonth, y = attendance)) +
  geom_line(stat = "identity") +
  labs(y = "Number of patients",
       title = "Number of patients who enter ED") +
  facet_wrap(vars(Aggregated_Organisation), scales = "free_y", ncol = 3) +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


**Analysing Health Board Attendance Trends Amidst and Post-COVID-19:**

Healthcare systems around the world faced significant difficulties during the COVID-19 epidemic, which caused them to quickly shift into crisis-response mode. Intriguing trends during and after the epidemic era have been revealed by a thorough review of attendance data from local health boards.

- During the pandemic:

Surprisingly, several local health boards experienced a large reduction in attendance throughout the epidemic period. This significant drop in attendance, which may have been brought on by reasons including stringent public health regulations, widespread concern over virus exposure, and maybe changes in healthcare delivery strategies, was an anomalous departure from historical attendance trends.

- Post pandemic:

After the pandemic's intensity started to lessen, there was a noticeable increase in attendance, however there were notable differences amongst the local health boards. In contrast to most boards, which showed a strong recovery and raised their attendance figures to levels prior to the pandemic, Powys Teaching stood out as an outlier and defied this recovery trend. To fully understand the subtleties affecting these disparate courses and influence upcoming strategic planning, more research is necessary.

- Findings on seasonality:

Initial data analysis reveals the existence of seasonality, which is characterized by repeated swings in attendance across all health boards. This pattern calls for a perceptive investigation into:

Identify Underlying Causes: 
Researching seasonality's probable triggers, such as public health initiatives, flu seasons, or holiday seasons, in order to comprehend the factors influencing these patterns.

Determine Seasonal influence: 
Establishing whether the observed seasonality has an equal influence on all health boards or whether there are differences, which may be a sign of regional causes or policy changes.

Planning and Forecasting: 
By using this understood seasonality to improve forecasting models and ensuring that they are calibrated to account for these seasonal changes, more precise and useful predictive insights are made possible.


\newpage

## Seasonality of number of attendances

To further explore whether there was any trend and/or seasonality in the data by LHB, the time series was decomposed using the STL method and the extracted components (trend, seasonal and residuals) plotted:

```{r}
data_grouped |> 
  group_by(Aggregated_Organisation) |>
  summarise(`Number of patients` = sum(Data)) |>
  gg_season() +
  facet_wrap(vars(Aggregated_Organisation), scales = "free_y", nrow = 3)+
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Seasonal plots of ED Attendances (by LHB)")
```
- Findings:

1. Most LHBs have a trend whereby attendance is lower in some months, like April and November, and higher in others, like June and July.

2. In many of the LHBs, the attendance in 2020 differs noticeably from other years.
In comparison to similar months in past years, the majority of LHBs appear to have a fall or reduction in ED attendances during several months of 2020. This could be a sign of outside influences influencing access to or use of healthcare because of the pandemic.
3. 
- Aneurin Bevan: The middle of the year for 2020 shows a clear decline.
- Cardiff & Vale: Attendances in 2020 appear to be continuously lower than in previous years, particularly in the early months.
- Hywel Dda: Around mid-year, there is a noticeable decline in attendance.
- Betsi Cadwaladr: While the trend for 2020 appears to be largely constant, several months still show a decline in attendance.
- Grouped_4_organisation: Attendance clearly drops off during particular months.
- Powys Teaching: The variations are more pronounced, with obvious dips during particular months.



### Decompose Time Series

```{r}

# Decompose time series for each health board using STL
stl_decompositions <- data2_hts %>%
  split(.$Aggregated_Organisation) %>%
  purrr::map(function(data){
    ts_data <- ts(data$attendance, frequency = 12)
    stl(ts_data, s.window = "periodic")
  })

# Convert the decompositions to a tidy data frame
stl_df <- purrr::map2_dfr(stl_decompositions, names(stl_decompositions), 
                          ~{
                            time_series <- as.data.frame(.x$time.series)
                            time_series$Aggregated_Organisation <- .y
                            return(time_series)
                          })

```


```{r}
num_months <- length(unique(data2_hts$YearMonth))
num_orgs <- length(unique(data2_hts$Aggregated_Organisation))

# Correct assignment for YearMonth column
stl_df$YearMonth <- rep(unique(data2_hts$YearMonth), times = num_orgs)

```

### Plotting

```{r}
stl_df_tidy <- stl_df %>%
  tidyr::pivot_longer(cols = c(trend, seasonal, remainder), 
                      names_to = "component", 
                      values_to = "value")

```


```{r}
stl_df_tidy %>%
  ggplot(aes(x = YearMonth, y = value, color = component)) +
  geom_line() +
  facet_wrap(vars(Aggregated_Organisation), scales = "free_y", ncol = 3) +
  labs(title = "STL Decomposition of ED Attendance",
       y = "Number of Patients",
       color = "Component") +
  scale_color_manual(values = c("blue", "green", "red"),
                     breaks = c("trend", "seasonal", "remainder"),
                     labels = c("Trend", "Seasonal", "Residual")) +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1))

```

Findings:

1. Trend:
There appears to be an upward trend in the number of ED visits over time for all LHBs, but there are outliers.
The most obvious decline occurs around 2020, which may be a sign of the COVID-19 pandemic's effects.

2. Seasonality:
The seasonal component illustrates the periodicity in ED attendances by showing predictable patterns that repeat annually.
There is a pronounced surge in the number of patient's attendance in the middle of the year (approximately in June or July). This seasonal pattern underlines the recurrent nature of patient admissions.

3. Residuals:
The residuals are the data's unexplained variation after the trend and seasonal components have been removed.
The majority of LHBs have residuals that oscillate near zero, which shows that the STL decomposition has effectively caught the main trends in the data.





\newpage

## Change the Age_Code structure into different groups for simplicity and interpretability

```{r}
unique(data_grouped$Age_Code)
```


### Age group: "0-4", "5-17", "18-69", "70^"
```{r}
data_grouped_age <- data_grouped %>%
  filter(Age_Code != "Unknown") %>%
  mutate(Grouped_Age = case_when(
    Age_Code == "0 to 4" ~ "0-4",
    Age_Code == "5 to 17" ~ "5-17",
    Age_Code %in% c("18 to 24", "25 to 29", "30 to 34", "35 to 39", 
                    "40 to 44", "45 to 49", "50 to 54", "55 to 59", 
                    "60 to 64", "65 to 69") ~ "18-69",
    Age_Code %in% c("70 to 74", "75 to 79", "80 to 84", "85") ~ "70 and over",
    TRUE ~ "Other"
  ))

```


## Plot Number of Patients in different age groups
```{r}
data_gts <- data_grouped_age %>%
  filter(Sex_ItemName_ENG != "Not Specified or invalid") %>%
  group_by(Grouped_Age, Sex_ItemName_ENG) %>%
  summarize(Number = sum(Data, na.rm = TRUE))

ggplot(data_gts, aes(x = Grouped_Age, y = Number)) +
  geom_bar(stat = "identity") +
  labs(y = "Number of patients") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~ Sex_ItemName_ENG, scales = "free")


```


Findings:

- The observation that the age group 18-69 has the most amount of patient attendance is expected, as it is the biggest group among all. However, it is noteworthy that the second biggest group are from the oldest age bracket, aligning with the general understanding of the health care need for the elders.

```{r}
#Change the data into wide format
data2_wide <- data_grouped %>%
  group_by(Aggregated_Organisation) %>%
  index_by(YearMonth) %>%
  summarise(attendance = sum(Data)) %>%
  pivot_wider(names_from = Aggregated_Organisation, values_from = attendance)


```

```{r}
data2_wide <- as_tibble(data2_wide)
data2_wide <- data2_wide %>%
   mutate(Total = rowSums(select(., c("Aneurin Bevan", "Betsi Cadwaladr", "Cardiff & Vale", "Grouped_4_organisation", "Hywel Dda", "Powys Teaching")), na.rm = TRUE))

```



# Forecast with ARIMA



```{r}

# Define the forecast horizon and validation period
h <- 12
validation_period <- 6

```



```{r}
# Splitting the data into training and testing sets
training_data <- head(data2_wide, nrow(data2_wide) - validation_period)
test_data <- tail(data2_wide, validation_period)

```


```{r}

cols_to_forecast <- c("Aneurin Bevan", "Betsi Cadwaladr", "Cardiff & Vale", "Grouped_4_organisation", "Hywel Dda", "Powys Teaching", "Total")
forecast_list <- list()

arima_forecast_list <- list()
start_year <- year(min(training_data$YearMonth))
start_month <- month(min(training_data$YearMonth))

for (col in cols_to_forecast) {
  ts_data <- ts(training_data[[col]], start = c(start_year, start_month), frequency = 12)
  arima_model <- auto.arima(ts_data)
  forecast_list[[col]] <- forecast(arima_model, h = h)
}
```

## Plotting using ARIMA

```{r  fig.height=4}

for (col in cols_to_forecast) {
  end_year_train <- year(max(training_data$YearMonth))
  end_month_train <- month(max(training_data$YearMonth))
  
  start_year_test <- ifelse(end_month_train == 12, end_year_train + 1, end_year_train)
  start_month_test <- ifelse(end_month_train == 12, 1, end_month_train + 1)
  
  actual_ts <- ts(test_data[[col]], start = c(start_year_test, start_month_test), frequency = 12)
  
  plot_forecast <- autoplot(forecast_list[[col]]) +
    autolayer(actual_ts, series="Actual", PI=FALSE) +
    labs(title = paste("Forecast vs Actual for", col)) +
    theme(legend.position = "bottom")
  
  print(plot_forecast)
}
```

# Forecasting with ETS

```{r}

ets_forecast_list <- list()
start_year <- year(min(training_data$YearMonth))
start_month <- month(min(training_data$YearMonth))

for (col in cols_to_forecast) {
  ts_data <- ts(training_data[[col]], start = c(start_year, start_month), frequency = 12)
  ets_model <- ets(ts_data)  # Use the ets function
  forecast_list[[col]] <- forecast(ets_model, h = h)
}


```

## Plotting with ets
```{r  fig.height=4}
for (col in cols_to_forecast) {
  end_year_train <- year(max(training_data$YearMonth))
  end_month_train <- month(max(training_data$YearMonth))
  
  start_year_test <- ifelse(end_month_train == 12, end_year_train + 1, end_year_train)
  start_month_test <- ifelse(end_month_train == 12, 1, end_month_train + 1)
  
  actual_ts <- ts(test_data[[col]], start = c(start_year_test, start_month_test), frequency = 12)
  
  plot_forecast <- autoplot(forecast_list[[col]]) +
    autolayer(actual_ts, series="Actual", PI=FALSE) +
    labs(title = paste("ETS Forecast vs Actual for", col)) +
    theme(legend.position = "bottom")
  
  print(plot_forecast)
}

```





- Conformity:

Hywel Dda, Powys Teaching, Cardiff & Vale, Grouped_4_Organisation, and Total (All-Wales): For these health boards, the ARIMA model effectively predicted the downturn that was observed in the latter portions of 2023 and successfully reflected the actual data, indicating a cogent direction. This demonstrates ARIMA's ability to effectively model and anticipate time-series data for these specific entities, despite the unpredictably fluctuating conditions brought on by the epidemic.

- Contradictions:

Aneurin Bevan and Betsi Cadwaladr: By contrast, the prediction statistics for Aneurin Bevan and Betsi Cadwaladr show a clear departure from the actual numbers. It is necessary to assess the ARIMA model's applicability and accuracy with regard to these specific datasets because there is a discrepancy between the predicted value and the real-time data.


# Forecasting with ETS

```{r}

ets_forecast_list <- list()
start_year <- year(min(training_data$YearMonth))
start_month <- month(min(training_data$YearMonth))

for (Aggregated_Organisation in cols_to_forecast) {
  ts_data <- ts(training_data[[Aggregated_Organisation]], start = c(start_year, start_month), frequency = 12)
  ets_model <- ets(ts_data) 
  forecast_list[[Aggregated_Organisation]] <- forecast(ets_model, h = h)
}


```



## Plotting with ets
```{r  fig.height=4}
for (col in cols_to_forecast) {
  end_year_train <- year(max(training_data$YearMonth))
  end_month_train <- month(max(training_data$YearMonth))
  
  start_year_test <- ifelse(end_month_train == 12, end_year_train + 1, end_year_train)
  start_month_test <- ifelse(end_month_train == 12, 1, end_month_train + 1)
  
  actual_ts <- ts(test_data[[col]], start = c(start_year_test, start_month_test), frequency = 12)
  
  plot_forecast <- autoplot(forecast_list[[col]]) +
    autolayer(actual_ts, series="Actual", PI=FALSE) +
    labs(title = paste("ETS Forecast vs Actual for", col)) +
    theme(legend.position = "bottom")
  
  print(plot_forecast)
}

```


Upon first glance, it appears that the ETS model offers a relatively close alignment with the real data, demonstrating a strong predictive power. When compared to the ARIMA model, the projections closely mimic the actual data patterns rather than merely following them, which suggests a possibility for greater accuracy.

-Consistent Decline in Predictions:

Downtrend in 2023: Regardless of the model used, all forecasts uniformly point to a decline in trends during the second half of 2023. This might be explained by the historical data's apparent cyclical nature. In other words, the historical data reveals a pattern in which peaks frequently appear in the middle of the year and then descend into troughs at year's end and the beginning of the next year. This recurrent pattern may have a significant impact on the expected downturn in 2023.

In light of the above, it is crucial to approach model selection with a nuanced comprehension of the underlying data and impacting factors, even though the ETS model shows a potentially higher prediction accuracy within the given circumstances. 

An accuracy analysis was then carried out upon for both ARIMA and ETS models to further assess the quality of their predictions.

# Accuracy assessment for ARIMA and ETS


```{r}
# Lists to store forecasts and error metrics
arima_forecast_list <- list()
ets_forecast_list <- list()
arima_error_metrics <- list()
ets_error_metrics <- list()
```


```{r}
# Forecasting with ARIMA and ETS
for (col in cols_to_forecast) {
  ts_data <- ts(training_data[[col]], start = c(start_year, start_month), frequency = 12)
  
  # ARIMA model
  arima_model <- auto.arima(ts_data)
  arima_forecast <- forecast(arima_model, h = h)
  arima_forecast_list[[col]] <- arima_forecast
  
  # ETS model
  tryCatch({
    ets_model <- ets(ts_data)
    ets_forecast <- forecast(ets_model, h = h)
    ets_forecast_list[[col]] <- ets_forecast
  }, error = function(e) {
    cat("Error in forecasting for column:", col, "with message:", e$message, "\n")
  })
}
```

## Calculating Accuracy Metrics for ARIMA and ETS
```{r}

for (col in cols_to_forecast) {
  actual <- test_data[[col]]
  
  arima_forecasted <- head(arima_forecast_list[[col]]$mean, validation_period)
  
  # ARIMA metrics
  arima_MAE <- mae(actual, arima_forecasted)
  arima_RMSE <- rmse(actual, arima_forecasted)
  arima_MAPE <- tryCatch(mape(actual, arima_forecasted), error = function(e) NA)
  arima_error_metrics[[col]] <- list(MAE = arima_MAE, RMSE = arima_RMSE, MAPE = arima_MAPE)
  
  # Check if ETS forecast exists for the column to avoid errors
  if (!is.null(ets_forecast_list[[col]])) {
    ets_forecasted <- head(ets_forecast_list[[col]]$mean, validation_period)
    
    # ETS metrics
    ets_MAE <- mae(actual, ets_forecasted)
    ets_RMSE <- rmse(actual, ets_forecasted)
    ets_MAPE <- tryCatch(mape(actual, ets_forecasted), error = function(e) NA)
    ets_error_metrics[[col]] <- list(MAE = ets_MAE, RMSE = ets_RMSE, MAPE = ets_MAPE)
  }
}

```

### Displaying Accuracy Metrics for each Column
```{r}
TableAccuracyMetrics <-
  rbind(
    # aggregate ARIMA accuracy metrics
    as.data.frame(
      do.call(rbind,unlist(arima_error_metrics, recursive=FALSE))
      ) |>
      rename(Metric = V1) |>
      rownames_to_column("LHB") |>
      mutate(
        Metric = round(Metric,3),
        Model = "ARIMA"
        ),
    
    # aggregate ETS accuracy metrics
    as.data.frame(
      do.call(rbind,unlist(ets_error_metrics, recursive=FALSE))
      ) |>
      rename(Metric = V1) |>
      rownames_to_column("LHB") |>
      mutate(
        Metric = round(Metric,3),
        Model = "ETS"
        )
    ) |>
  
  ## wrangle table from long to wide format
  tidyr::pivot_wider(
    names_from = Model, 
    values_from = Metric
  )

TableAccuracyMetrics |> knitr::kable()
```





- With Cardiff & Vale being a significant exception, most places displayed reduced error metrics when modeled with ETS, signifying a higher predictive accuracy as compared to the ARIMA model.

- Model Differentiation: 

ARIMA, A Time-Series Tradition Autoregressive, integrative (difference), and moving average features are included in the forecasting core components.
The foundation is laid by differencing to make data stationary, and then autoregressive and moving average techniques are applied.

ETS: A Comprehensive Method for Time-Series Model Constituent Elements reflects the seasonality, trend, and error components in time series data. Consists of rigorous detection and modeling of error, trend, and seasonal patterns within the data, which tends to include entire pattern analysis.


- According to the interest expressed by the NHS executives, I would like to provide some insights for the error metrics according to the quality of the forecast for the non-zero count data:


1. **Root Mean Squared Error (RMSE)**
   - **Description**: Measures the square root of the average squared differences between forecast and actual values.
   - **Pros**: It gives more weight to large errors.
   - **Cons**: It can be influenced significantly by outliers. RMSE might be inflated, if the count data is prone to significant spikes or declines, which should not be the problem for this data except for the Covid-19 era.
   
2. **Mean Absolute Percentage Error (MAPE)**
   - **Description**: Measures the average of the absolute percentage errors.
   - **Pros**: It is a relative metric, and is scale-independent and easy to interpret.
   - **Cons**: MAPE can overemphasize relative errors on small counts.
   
3. **Mean Absolute Error (MAE)**
   - **Description**: Measures the average of the absolute differences between forecast and actual values.
   - **Pros**: It's less sensitive to outliers than RMSE. It provides a straightforward average error size.
   - **Cons**: It does not emphasize large errors.

**However, RMSE and MAE should not be used for the hierarchical time series data.**
And here is why:

- **1. Consistency Between Levels:**
Granularity Errors are not consistently distributed or similar across all hierarchical levels due to the variance in data granularity between levels, and there will be an aggregation dilemma. Due to the variation in magnitude and volume at various hierarchical levels, direct aggregation of RMSE or MAE from bottom levels to top levels might produce inconsistent or biased judgments of forecast accuracy.

- **2. Problems with reconciliation:**
It is possible for forecasts to not add up coherently in a hierarchical structure when they are independently generated for several levels. Since they assess mistakes without taking into account hierarchical dependencies, RMSE and MAE do not by default address this reconciliation. And the alignment issue will therefore be caused. The use of standard error measurements at various hierarchical levels may produce false conclusions regarding the precision and dependability of the forecasts in the absence of a cogent reconciliation process.

- **3. Metric incomparability:**
Impact of a unit error is not constant across levels; for instance, a 100-unit error may be insignificant at a higher aggregate level but significant at a lower one. Additionally, due to the divergent scales of the errors at various levels, it is difficult to use a single statistic, such as RMSE or MAE, to fairly compare performance across all hierarchical levels.




# Reconciliation

## Aggregate data

```{r}
# hierarchy with age group
data_hts <- data_grouped_age |>
  aggregate_key(Aggregated_Organisation / Hospital_ItemName_ENG * Grouped_Age, attendance = sum(Data))
```



```{r}
# hierarchy without age group, only aggregate with the organisation, wales as a big picture
data_wales <- data_grouped_age |>
  aggregate_key(Aggregated_Organisation, attendance = sum(Data))
```




## Time series cross validation
## stretch_tsibble(.init = 135-36, .step = 1)

```{r}
#hts
library(tscount)
# Split data
training_data <- data_hts %>% 
  filter(YearMonth <= yearmonth("2020 Jun"))

test_data <- data_hts %>% 
  filter(YearMonth > yearmonth("2020 Jun")) 

test_data_filtered <- test_data %>%
  filter(Aggregated_Organisation == "<aggregated>")

```


```{r}
# average the three models
library(fable.tscount)
fit_cv<- training_data %>%
  model(
    naive_model = NAIVE(attendance),
    ets_model = ETS(attendance),
    tscount = TSCOUNT(attendance ~ trend() + season() , link = "log", model = list(past_obs = 1:12))
  )|> mutate (comb = (naive_model+ets_model+tscount)/3) 
```



```{r}
# Generate forecasts
fc_cv <- fit_cv %>%
  forecast(h = 36) %>%
  filter(.model == "comb") 

```


```{r}
# Generate forecasts for aggregated
fc_cv_filtered <- fit_cv %>%
  forecast(h = 36) %>%
  filter(.model == "comb") %>%
  filter(Aggregated_Organisation == "<aggregated>")
```


```{r accuracy_needs_to_be_updated}
# Check the accuracy
forecasted_values <- fc_cv_filtered$.mean
actual_values <- test_data_filtered$attendance
actual_values <- as.numeric(actual_values)

actual_values_models <- test_data$attendance

```

```{r not_sure_if_it_should_be_calc_like_this1}

errors <- actual_values - forecasted_values
mean_abs_error <- mean(abs(errors), na.rm = TRUE)
naive_forecast_error <- mean(abs(diff(actual_values)), na.rm = TRUE)

MASE <- mean_abs_error / naive_forecast_error
print(MASE)

```


```{r not_sure_if_it_should_be_calc_like_this2}
mean_squared_error <- mean(errors^2, na.rm = TRUE)
naive_forecast_squared_error <- mean(diff(actual_values)^2, na.rm = TRUE)

RMSSE <- sqrt(mean_squared_error / naive_forecast_squared_error)
print(RMSSE)

```



```{r}

# Function to calculate MASE and RMSSE for a single model
calc_scaled_errors_for_model_organisation <- function(model_name, organisation) {
 # Get forecasted and actual values for specific model and organisation
  fc_single_model_org <- fit_cv %>%
    forecast(h = 12) %>%
    filter(.model == model_name, Aggregated_Organisation == organisation)
  
  forecasted_values_single <- fc_single_model_org$.mean
  
  actual_values_org <- test_data %>%
    filter(Aggregated_Organisation == organisation) %>%
    pull(attendance)
  
  # Calculate errors
  errors <- actual_values_org - forecasted_values_single
  mean_abs_error <- mean(abs(errors), na.rm = TRUE)
  mean_squared_error <- mean(errors^2, na.rm = TRUE)
  
  # Calculate naive errors using direct indexing for lag
  naive_forecast <- c(NA, actual_values_org[1:(length(actual_values_org) - 1)]) # Shift the series by one
  naive_errors <- actual_values_org - naive_forecast
  naive_forecast_error <- mean(abs(naive_errors), na.rm = TRUE) # For MASE
  naive_forecast_squared_error <- mean(naive_errors^2, na.rm = TRUE) # For RMSSE
  
  # Calculate MASE and RMSSE
  MASE <- mean_abs_error / naive_forecast_error
  RMSSE <- sqrt(mean_squared_error / naive_forecast_squared_error) # This is a hypothesized version of RMSSE based on MASE's concept
  
  return(list(MASE = MASE, RMSSE = RMSSE))
}


# Specify models and organisations
models <- c("naive_model", "ets_model", "tscount", "comb")
organisations <- unique(training_data$Aggregated_Organisation)

# Nested lapply to iterate over models and organisations
results_list <- lapply(models, function(model) {
  model_results <- lapply(organisations, function(org) {
    calc_scaled_errors_for_model_organisation(model, org)
  })
  
  # Name each element in the list for clarity
  names(model_results) <- organisations
  
  # Include the model name in the results
  return(list(
    model = model,
    results = model_results
  ))
})

# Set names for the list based on model names for clarity
names(results_list) <- models

```




```{r}
# Define safe_extract function
safe_extract <- function(x, field) {
  if (is.list(x) && !is.null(x[[field]])) {
    return(x[[field]])
  } else {
    return(NA)
  }
}

# Convert the results list to a data frame
results_df <- map_df(results_list, ~{
  if (is.list(.x$results)) {
    map_df(.x$results, ~{
      tibble(
        organisation = names(.x), 
        MASE = safe_extract(.x, "MASE"), 
        RMSSE = safe_extract(.x, "RMSSE")
      )
    }, .id = "model") 
  } else {
    return(tibble(organisation = NA, MASE = NA, RMSSE = NA, model = NA))
  }
}, .id = "model_name") 

# Selecting relevant columns
results_df <- results_df %>%
  select(model_name, model, organisation, MASE, RMSSE) %>%
  filter(model == "<aggregated>")

# Print the results
print(results_df)

```

### tscount seems to be performing worse than the naive model, therefore, I would like to remove that from the models included.


## Remove tscount
```{r}
fit_cv_no_tscount<- training_data %>%
  model(
    naive_model = NAIVE(attendance),
    ets_model = ETS(attendance)
  )|> mutate (comb = (naive_model+ets_model)/2) 

```



```{r}
# Generate forecasts
fc_cv_no_tscount <- fit_cv_no_tscount %>%
  forecast(h = 36) 

```




```{r}

# Function to calculate MASE and RMSSE for a single model
calc_scaled_errors_for_model_organisation <- function(model_name, organisation) {
 # Get forecasted and actual values for specific model and organisation
  fc_single_model_no_tscount <- fit_cv_no_tscount %>%
    forecast(h = 12) %>%
    filter(.model == model_name, Aggregated_Organisation == organisation)
  
  forecasted_values_no_tscount <- fc_single_model_no_tscount$.mean
  
  actual_values_org <- test_data %>%
    filter(Aggregated_Organisation == organisation) %>%
    pull(attendance)
  
  # Calculate errors
  errors <- actual_values_org - forecasted_values_no_tscount
  mean_abs_error <- mean(abs(errors), na.rm = TRUE)
  mean_squared_error <- mean(errors^2, na.rm = TRUE)
  
  # Calculate naive errors using direct indexing for lag
  naive_forecast <- c(NA, actual_values_org[1:(length(actual_values_org) - 1)]) # Shift the series by one
  naive_errors <- actual_values_org - naive_forecast
  naive_forecast_error <- mean(abs(naive_errors), na.rm = TRUE) # For MASE
  naive_forecast_squared_error <- mean(naive_errors^2, na.rm = TRUE) # For RMSSE
  
  # Calculate MASE and RMSSE
  MASE <- mean_abs_error / naive_forecast_error
  RMSSE <- sqrt(mean_squared_error / naive_forecast_squared_error) # This is a hypothesized version of RMSSE based on MASE's concept
  
  return(list(MASE = MASE, RMSSE = RMSSE))
}


# Specify models and organisations
models <- c("naive_model", "ets_model", "tscount", "comb")
organisations <- unique(training_data$Aggregated_Organisation)

# Nested lapply to iterate over models and organisations
results_list <- lapply(models, function(model) {
  model_results <- lapply(organisations, function(org) {
    calc_scaled_errors_for_model_organisation(model, org)
  })
  
  # Name each element in the list for clarity
  names(model_results) <- organisations
  
  # Include the model name in the results
  return(list(
    model = model,
    results = model_results
  ))
})

# Set names for the list based on model names for clarity
names(results_list) <- models

```



```{r}
# Convert the results list to a data frame
results_df <- map_df(results_list, ~{
  if (is.list(.x$results)) {
    map_df(.x$results, ~{
      tibble(
        organisation = names(.x), 
        MASE = safe_extract(.x, "MASE"), 
        RMSSE = safe_extract(.x, "RMSSE")
      )
    }, .id = "model") 
  } else {
    return(tibble(organisation = NA, MASE = NA, RMSSE = NA, model = NA))
  }
}, .id = "model_name") 

# Selecting relevant columns
results_df <- results_df %>%
  select(model_name, model, organisation, MASE, RMSSE) %>%
  filter(model == "<aggregated>")

# Print the results
print(results_df)

```


```{r no_tscount}
joined_data_no_tscount <- as_tibble(test_data) %>%
  left_join(as_tibble(fc_cv_no_tscount), by = c("YearMonth", "Aggregated_Organisation")) %>%
  select(-attendance.y) %>%
  rename(attendance = attendance.x) %>%
  rename(forecast = .mean)
```




joined_data <- as_tibble(test_data) %>%
  left_join(as_tibble(fc_cv), by = c("YearMonth", "Aggregated_Organisation")) %>%
  select(-attendance.y) %>%
  rename(attendance = attendance.x) %>%
  rename(forecast = .mean)



joined_data_long<- joined_data %>%
  pivot_longer(
    cols = c(attendance, forecast),
    names_to = "Type",
    values_to = "Value"
  )



unique_organisations <- unique(joined_data_long$Aggregated_Organisation)

plot_list <- map(unique_organisations, ~{
  ggplot(joined_data_long[joined_data_long$Aggregated_Organisation == .x,], 
         aes(x = YearMonth, y = Value, color = Type)) +
    geom_line() +
    labs(title = paste("Forecasted vs Actual Values for", .x),
         y = "Values") +
    theme_minimal()
})
print(plot_list)


# Running t-tests and extracting p-value and t-value
t_test_summary <- joined_data %>%
  summarise(
    t_value = list(t.test(attendance, forecast)$statistic),
    p_value = list(t.test(attendance, forecast)$p.value)
  ) %>%
  unnest(cols = c(t_value, p_value))

print(t_test_summary)


The p value is relatively high. It suggests that we fail to reject the null hypothesis, meaning there is no enough evidence to conclude that forecast and actual values are significantly different. In other words, the t-value of 0.313 is not large enough to be considered statistically significant.




- Base Forecasting: This technique is used to forecast straightforward time series data. Using statistical or machine learning models to extrapolate future data points based on observed past data is the main focus of this technique. Base forecasting, which is frequently applied when dealing with a single time series, tries to find patterns, trends, and seasonality in the historical data in order to produce precise and trustworthy future projections.

- Reconciliation Forecasting (Hierarchical Forecasting): Reconciliation or Hierarchical forecasting is typically used when managing hierarchical or grouped time series data. It involves generating forecasts on several levels of aggregation. This process not only creates projections for each individual time series, but also reconciles the forecasts to guarantee consistency throughout the hierarchy's many levels. The forecasts may be distributed across hierarchical levels using a variety of strategies, such as top-down, bottom-up, to make sure that the projections at each level coincide with their corresponding aggregate levels. When the data contains a hierarchical structure, such as organisational, product, or geographic hierarchies, hierarchical forecasting is especially advantageous since it ensures consistent and consolidated projections at all levels.





## Reconcile

### 1. Fitting models and Forecasting without tscount



```{r}
fit_wales <- data_wales |>
  model(
    naive_model = NAIVE(attendance),
    ets_model = ETS(attendance),
  )|> mutate (comb = (naive_model+ets_model)/2)
```
  
```{r}
wales_rc <- fit_wales %>%
   reconcile(bu = bottom_up(comb),
            wls_ETS1 = min_trace(comb, method = "wls_struct"),
            wls_ETS2 = min_trace(comb, method = "wls_var"),
            mint_ETS = min_trace(comb, method = "mint_shrink")) %>%
  forecast(h = 6)
```


```{r}
forecast_wales_avg <- wales_rc %>%
  as_tibble() %>%
  group_by(Aggregated_Organisation, YearMonth) %>%
  summarise(mean_attendance = mean(.mean, na.rm = TRUE), .groups = "drop")

```

```{r}
forecast_wales_avg <- forecast_wales_avg %>%
  rename(attendance = mean_attendance)
```

```{r}
data_wales$Aggregated_Organisation <- as.factor(data_wales$Aggregated_Organisation)
data2_hts$Aggregated_Organisation <- as.factor(data2_hts$Aggregated_Organisation)
forecast_wales_avg$Aggregated_Organisation <- as.factor(forecast_wales_avg$Aggregated_Organisation)

combined_data_rc <- bind_rows(data_wales, data2_hts, forecast_wales_avg)

```




```{r}

combined_data_rc <- combined_data_rc %>%
  mutate(data_type = if_else(YearMonth < ymd("2023-06-01"), "Historical", "Forecast"))


# Plotting
ggplot(combined_data_rc, aes(x = YearMonth, y = attendance, color = data_type)) +
  geom_line() +
  facet_wrap(~Aggregated_Organisation, scales = "free_y") +
  theme_minimal() +
  labs(title = "Attendance Over Time",
       subtitle = "Distinguishing between historical and forecasted data",
       x = "Year and Month",
       y = "Attendance",
       color = "Data Type") +
  scale_color_manual(values = c("Historical" = "blue", "Forecast" = "red"))

```




## Include Age Group



```{r}
data_full <- data_grouped_age |>
  aggregate_key((Aggregated_Organisation/Hospital_ItemName_ENG) * Grouped_Age, attendance = sum(Data))

filter_date <- yearmonth("2022 Jun")

data_full <- data_full %>%
  fill_gaps(.full = TRUE)
```



```{r}
fit_age <- data_full %>%
  filter(yearmonth(YearMonth) <= filter_date) %>%
  model(
naive_model = NAIVE(attendance),
    ets_model = ETS(attendance)
  )|> mutate (comb = (naive_model+ets_model)/2)
```
```{r}


# Remove groups with all NAs
data_full_no_all_na <- data_full %>%
  group_by(Aggregated_Organisation, Hospital_ItemName_ENG, Grouped_Age) %>%
  filter(!all(is.na(attendance))) %>%
  ungroup()

# For illustration, we're using na.locf from zoo to fill NA values (Last Observation Carried Forward).
# Depending on the nature of your data, you might need a different imputation method.
library(zoo)
data_full_no_na <- data_full_no_all_na %>%
  mutate(attendance = na.locf(attendance, na.rm = FALSE))

# Try fitting the models again
fit_age <- data_full_no_na %>%
  filter(yearmonth(YearMonth) <= filter_date) %>%
  model(
    naive_model = NAIVE(attendance),
    ets_model = ETS(attendance)
  ) %>%
  mutate(comb = (naive_model + ets_model) / 2)

```

```{r}
fc_age <- fit_age %>%
  select("comb") %>%
  reconcile(bu = bottom_up(comb),
            ols = min_trace(comb, method = "ols")) %>%
  forecast(h=18)
```


```{r}

# Filter the forecast results for aggregated levels and plot
fc_age %>%
  filter(is_aggregated(Aggregated_Organisation), !is_aggregated(Grouped_Age)) %>%
  autoplot(
    data_full %>% filter(yearmonth(YearMonth) >= yearmonth("2020 Jan")),level = NULL
  ) +
  labs(y = "Attendance ('000)") +
  facet_wrap(vars(Grouped_Age), scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 0, hjust = 1))

```






```{r}
library(igraph)

# Nodes data
nodes_df <- data.frame(
  name = c("All Wales", 
           "Betsi Cadwaladr", "Hywel Dda", "Grouped_4_organisation", 
           "Cardiff & Vale", "Aneurin Bevan", "Powys Teaching", 
           "Ysbyty Glan Clwyd", "Wrexham Maelor Hospital", "Colwyn Bay Community Hospital", 
           "Holywell Community Hospital", "Mold Community Hospital"),
  stringsAsFactors = FALSE
)

# Edges data
edges_df <- data.frame(
  from = c(rep("All Wales", 6), 
           rep("Betsi Cadwaladr", 5)),
  to = c("Betsi Cadwaladr", "Hywel Dda", "Grouped_4_organisation", 
         "Cardiff & Vale", "Aneurin Bevan", "Powys Teaching",
         "Ysbyty Glan Clwyd", "Wrexham Maelor Hospital", "Colwyn Bay Community Hospital", 
         "Holywell Community Hospital", "Mold Community Hospital"),
  stringsAsFactors = FALSE
)

# Creating graph object
graph <- graph_from_data_frame(d = edges_df, vertices = nodes_df, directed = TRUE)

# Color vector: 1 color for "All Wales", 6 colors for organisations, and 5 colors for hospitals
vertex_colors <- c("skyblue", rep("lightgreen", 6), rep("lightpink", 5))

# Plot the graph
plot(graph, 
     vertex.size = 15, 
     vertex.label.cex = 0.6,  
     vertex.color = vertex_colors,  
     edge.arrow.size = 0.5, 
     vertex.label.degree = pi/4, # 45-degree label rotation
     layout = layout.reingold.tilford(graph, root = 1))

```

