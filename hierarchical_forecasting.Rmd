---
title: "hierarchical_forecasting_report"
author: "Janice Hsu"
date: "2023-09-30"
output:
  bookdown::pdf_document2:
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE,
                      fig.pos = "H", out.extra = "")
```

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```


\newpage

# Preliminary Analysis

```{r}
# Required Libraries
library(zoo)
library(ggplot2)
library(lubridate)
library(tsibble)
library(tidyverse)
library(fpp3)
library(hts)
library(dplyr)
library(tidyr)
library(forecast)
library(Metrics)
library(purrr)
library(tidyr)
library(stats)
library(fable)
library(dplyr)



```


```{r}
data <- read.csv("HLTH0037_ts_cleaned.csv")
```


## Data Introduction

The dataset contains 10 variables related to the hospitals and information of patients in Wales, UK. Here are the brief summary of the dataset:

- Data: This column represents the number of attendance in each emergency department.

- YearMonth: This column represents dates in the year-month format. Additionally, this dataset contains data from 2012 April to 2023 May.

- Age_Code: This column provides the age group that the patient is in. There are 17 different age groups. They are "0 to 4","5 to 17","18 to 24","25 to 29","30 to 34","35 to 39","40 to 44","45 to 49","50 to 54","55 to 59","60 to 64","65 to 69","70 to 74","75 to 79","80 to 84","85" and "Unknown".

- Sex_ItemName_ENG: This column provides the information of patient's gender. 

- Hospital_Code: This column represents 42 different hospitals in Wales.

- Hospital_ItemName_ENG: This columns refers to the name of the 42 different hospitals in Wales.

- Hospital_Hierarchy: This column represents the code for the health board that the hospital belongs to.

- Hospital_AltCode1: This column provides an alternate code for the hospital.

- Organisation: This column represents the health board.

- Organisation_Code: A code for the organisation as well as the health board.

- There are three hierarchies in this dataset. On the top level, there is all the hospitals in Wales, while on the second hierarchy, there are 6 different health boards which also shown as the organisations. At the bottom level, there are 42 hospitals in total.



\newpage
# Exploratory Data Analysis

```{r}
# change data structure
data <- data %>%
  mutate(YearMonth = yearmonth(YearMonth)) %>%
  as_tsibble(index = YearMonth, key = c(Age_Code, Sex_ItemName_ENG, Hospital_Code, Hospital_ItemName_ENG)) 

```

## Number of patients entering ED under different hospital hierarchy
```{r}
# Aggregate the data
data_hts <- data %>%
  aggregate_key(Organisation/Hospital_ItemName_ENG, attendance = sum(Data))

# Plot the aggregated data

data_hts |>
  filter(is_aggregated(Hospital_ItemName_ENG)) |>
  autoplot(attendance) +
  labs(y = "Number of patients",
       title = "Number of patients who enter ED") +
  facet_wrap(vars(Organisation), scales = "free_y", ncol = 3) +
  theme(legend.position = "none")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 0, hjust = 1)) 
```

-	A couple of Local Health Boards (LHBs) were redefined from the 1st of April 2019 onwards: Cwm Taf (27)--> Cwm Taf Morgannwg (30)// Abertawe Bro Morgannwg (26) --> Swansea Bay (31). Therefore, if you decide to forecast at LHB resolution, you might want to consider these 4 as a unique one. 

-	A the Princess of Wales Hospital changed its Local Health Boards

-	So we analyse these 4 as one organisation

\newpage

## Group the changed Local Health Board together

```{r}
# mutate Aggregated_Organisation due to the change of the health boards

data_grouped <- data %>%
  mutate(Aggregated_Organisation = case_when(
    Organisation %in% c("Cwm Taf", "Cwm Taf Morgannwg", "Abertawe Bro Morgannwg", "Swansea Bay") ~ "Grouped_4_organisation",
    TRUE ~ Organisation
  ))

```

### There are 6 Local Health Boards
```{r}
unique(data_grouped$Aggregated_Organisation)
```

```{r}
data2_hts <- data_grouped %>%
  group_by(Aggregated_Organisation) %>%
  summarise(attendance = sum(Data)) 
```




## Number of patients who enter ED under 6 different local health boards
```{r}
data2_hts |>
  ggplot(aes(x = YearMonth, y = attendance)) +
  geom_line(stat = "identity") +
  labs(y = "Number of patients",
       title = "Number of patients who enter ED") +
  facet_wrap(vars(Aggregated_Organisation), scales = "free_y", ncol = 3) +
  theme(legend.position = "none") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```


**Analysing Health Board Attendance Trends Amidst and Post-COVID-19:**

Healthcare systems around the world faced significant difficulties during the COVID-19 epidemic, which caused them to quickly shift into crisis-response mode. Intriguing trends during and after the epidemic era have been revealed by a thorough review of attendance data from local health boards.

- During the pandemic:

Surprisingly, several local health boards experienced a large reduction in attendance throughout the epidemic period. This significant drop in attendance, which may have been brought on by reasons including stringent public health regulations, widespread concern over virus exposure, and maybe changes in healthcare delivery strategies, was an anomalous departure from historical attendance trends.

- Post pandemic:

After the pandemic's intensity started to lessen, there was a noticeable increase in attendance, however there were notable differences amongst the local health boards. In contrast to most boards, which showed a strong recovery and raised their attendance figures to levels prior to the pandemic, Powys Teaching stood out as an outlier and defied this recovery trend. To fully understand the subtleties affecting these disparate courses and influence upcoming strategic planning, more research is necessary.

- Findings on seasonality:

Initial data analysis reveals the existence of seasonality, which is characterized by repeated swings in attendance across all health boards. This pattern calls for a perceptive investigation into:

Identify Underlying Causes: 
Researching seasonality's probable triggers, such as public health initiatives, flu seasons, or holiday seasons, in order to comprehend the factors influencing these patterns.

Determine Seasonal influence: 
Establishing whether the observed seasonality has an equal influence on all health boards or whether there are differences, which may be a sign of regional causes or policy changes.

Planning and Forecasting: 
By using this understood seasonality to improve forecasting models and ensuring that they are calibrated to account for these seasonal changes, more precise and useful predictive insights are made possible.


\newpage

## Seasonality of number of attendances

To investigate deeper into the potential seasonality in the data, I would like to decompose the time series according the each health board. It allows us to discover the trend, seasonality and the residual components.



### Decompose Time Series

```{r}

# Decompose time series for each health board using STL
stl_decompositions <- data2_hts %>%
  split(.$Aggregated_Organisation) %>%
  purrr::map(function(data){
    ts_data <- ts(data$attendance, frequency = 12)
    stl(ts_data, s.window = "periodic")
  })

# Convert the decompositions to a tidy data frame
stl_df <- purrr::map2_dfr(stl_decompositions, names(stl_decompositions), 
                          ~{
                            time_series <- as.data.frame(.x$time.series)
                            time_series$Aggregated_Organisation <- .y
                            return(time_series)
                          })

```


```{r}
num_months <- length(unique(data2_hts$YearMonth))
num_orgs <- length(unique(data2_hts$Aggregated_Organisation))

# Correct assignment for YearMonth column
stl_df$YearMonth <- rep(unique(data2_hts$YearMonth), times = num_orgs)

```

### Plotting

```{r}
stl_df_tidy <- stl_df %>%
  tidyr::pivot_longer(cols = c(trend, seasonal, remainder), 
                      names_to = "component", 
                      values_to = "value")

```


```{r}
stl_df_tidy %>%
  ggplot(aes(x = YearMonth, y = value, color = component)) +
  geom_line() +
  facet_wrap(vars(Aggregated_Organisation), scales = "free_y", ncol = 3) +
  labs(title = "STL Decomposition of ED Attendance",
       y = "Number of Patients",
       color = "Component") +
  scale_color_manual(values = c("blue", "green", "red"),
                     breaks = c("trend", "seasonal", "remainder"),
                     labels = c("Trend", "Seasonal", "Residual")) +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 45, hjust = 1))

```
Based on the STL decomposition, it is evident that each health board follows a similar trend. Moreover, it is obvious that there is a significant downturn during the Covid-19 era. As for seasonality, there is a pronounced surge in the number of patient's attendance in the middle of the year (approximately in June or July). This seasoanl pattern underlines the recurrent nature of patient admissions.




\newpage

## Change the Age_Code structure into different groups for simplicity and interpretability

```{r}
unique(data_grouped$Age_Code)
```


### Age group: "0-4", "5-17", "18-69", "70^"
```{r}
data_grouped_age <- data_grouped %>%
  filter(Age_Code != "Unknown") %>%
  mutate(Grouped_Age = case_when(
    Age_Code == "0 to 4" ~ "0-4",
    Age_Code == "5 to 17" ~ "5-17",
    Age_Code %in% c("18 to 24", "25 to 29", "30 to 34", "35 to 39", 
                    "40 to 44", "45 to 49", "50 to 54", "55 to 59", 
                    "60 to 64", "65 to 69") ~ "18-69",
    Age_Code %in% c("70 to 74", "75 to 79", "80 to 84", "85") ~ "70 and over",
    TRUE ~ "Other"
  ))

```


## Plot Number of Patients in different age groups
```{r}
data_gts <- data_grouped_age %>%
  filter(Sex_ItemName_ENG != "Not Specified or invalid") %>%
  group_by(Grouped_Age, Sex_ItemName_ENG) %>%
  summarize(Number = sum(Data, na.rm = TRUE))

ggplot(data_gts, aes(x = Grouped_Age, y = Number)) +
  geom_bar(stat = "identity") +
  labs(y = "Number of patients") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  facet_wrap(~ Sex_ItemName_ENG, scales = "free")


```


Findings:

- The observation that the age group 18-69 has the most amount of patient attendance is expected, as it is the biggest group among all. However, it is noteworthy that the second biggest group are from the oldest age bracket, aligning with the general understanding of the health care need for the elders.
```{r}
#Change the data into wide format
data2_wide <- data_grouped %>%
  group_by(Aggregated_Organisation) %>%
  index_by(YearMonth) %>%
  summarise(attendance = sum(Data)) %>%
  pivot_wider(names_from = Aggregated_Organisation, values_from = attendance)


```

```{r}
data2_wide <- as_tibble(data2_wide)
data2_wide <- data2_wide %>%
   mutate(Total = rowSums(select(., c("Aneurin Bevan", "Betsi Cadwaladr", "Cardiff & Vale", "Grouped_4_organisation", "Hywel Dda", "Powys Teaching")), na.rm = TRUE))

```



# Forecast with ARIMA



```{r}

# Define the forecast horizon and validation period
h <- 12
validation_period <- 6

```



```{r}
# Splitting the data into training and testing sets
training_data <- head(data2_wide, nrow(data2_wide) - validation_period)
test_data <- tail(data2_wide, validation_period)

```


```{r}

cols_to_forecast <- c("Aneurin Bevan", "Betsi Cadwaladr", "Cardiff & Vale", "Grouped_4_organisation", "Hywel Dda", "Powys Teaching", "Total")
forecast_list <- list()

arima_forecast_list <- list()
start_year <- year(min(training_data$YearMonth))
start_month <- month(min(training_data$YearMonth))

for (col in cols_to_forecast) {
  ts_data <- ts(training_data[[col]], start = c(start_year, start_month), frequency = 12)
  arima_model <- auto.arima(ts_data)
  forecast_list[[col]] <- forecast(arima_model, h = h)
}
```

## Plotting using ARIMA

```{r  fig.height=4}

for (col in cols_to_forecast) {
  end_year_train <- year(max(training_data$YearMonth))
  end_month_train <- month(max(training_data$YearMonth))
  
  start_year_test <- ifelse(end_month_train == 12, end_year_train + 1, end_year_train)
  start_month_test <- ifelse(end_month_train == 12, 1, end_month_train + 1)
  
  actual_ts <- ts(test_data[[col]], start = c(start_year_test, start_month_test), frequency = 12)
  
  plot_forecast <- autoplot(forecast_list[[col]]) +
    autolayer(actual_ts, series="Actual", PI=FALSE) +
    labs(title = paste("Forecast vs Actual for", col)) +
    theme(legend.position = "bottom")
  
  print(plot_forecast)
}
```

# Forecasting with ETS

```{r}

ets_forecast_list <- list()
start_year <- year(min(training_data$YearMonth))
start_month <- month(min(training_data$YearMonth))

for (col in cols_to_forecast) {
  ts_data <- ts(training_data[[col]], start = c(start_year, start_month), frequency = 12)
  ets_model <- ets(ts_data)  # Use the ets function
  forecast_list[[col]] <- forecast(ets_model, h = h)
}


```

## Plotting with ets
```{r  fig.height=4}
for (col in cols_to_forecast) {
  end_year_train <- year(max(training_data$YearMonth))
  end_month_train <- month(max(training_data$YearMonth))
  
  start_year_test <- ifelse(end_month_train == 12, end_year_train + 1, end_year_train)
  start_month_test <- ifelse(end_month_train == 12, 1, end_month_train + 1)
  
  actual_ts <- ts(test_data[[col]], start = c(start_year_test, start_month_test), frequency = 12)
  
  plot_forecast <- autoplot(forecast_list[[col]]) +
    autolayer(actual_ts, series="Actual", PI=FALSE) +
    labs(title = paste("ETS Forecast vs Actual for", col)) +
    theme(legend.position = "bottom")
  
  print(plot_forecast)
}

```



Explanation for the ARIMA model:

In time series forecasting, the ARIMA (AutoRegressive Integrated Moving Average) model is frequently employed. It belongs to a class of models that can reproduce a variety of common temporal structures in time series data.


- Conformity:

Hywel Dda, Powys Teaching, Cardiff & Vale, Grouped_4_Organisation, and Total (All-Wales): For these health boards, the ARIMA model effectively predicted the downturn that was observed in the latter portions of 2023 and successfully reflected the actual data, indicating a cogent direction. This demonstrates ARIMA's ability to effectively model and anticipate time-series data for these specific entities, despite the unpredictably fluctuating conditions brought on by the epidemic.

- Contradictions:

Aneurin Bevan and Betsi Cadwaladr: By contrast, the prediction statistics for Aneurin Bevan and Betsi Cadwaladr show a clear departure from the actual numbers. It is necessary to assess the ARIMA model's applicability and accuracy with regard to these specific datasets because there is a discrepancy between the predicted value and the real-time data.


# Forecasting with ETS

```{r}

ets_forecast_list <- list()
start_year <- year(min(training_data$YearMonth))
start_month <- month(min(training_data$YearMonth))

for (Aggregated_Organisation in cols_to_forecast) {
  ts_data <- ts(training_data[[Aggregated_Organisation]], start = c(start_year, start_month), frequency = 12)
  ets_model <- ets(ts_data)  # Use the ets function
  forecast_list[[Aggregated_Organisation]] <- forecast(ets_model, h = h)
}


```



## Plotting with ets
```{r  fig.height=4}
for (col in cols_to_forecast) {
  end_year_train <- year(max(training_data$YearMonth))
  end_month_train <- month(max(training_data$YearMonth))
  
  start_year_test <- ifelse(end_month_train == 12, end_year_train + 1, end_year_train)
  start_month_test <- ifelse(end_month_train == 12, 1, end_month_train + 1)
  
  actual_ts <- ts(test_data[[col]], start = c(start_year_test, start_month_test), frequency = 12)
  
  plot_forecast <- autoplot(forecast_list[[col]]) +
    autolayer(actual_ts, series="Actual", PI=FALSE) +
    labs(title = paste("ETS Forecast vs Actual for", col)) +
    theme(legend.position = "bottom")
  
  print(plot_forecast)
}

```


Upon first glance, it appears that the ETS model offers a relatively close alignment with the real data, demonstrating a strong predictive power. When compared to the ARIMA model, the projections closely mimic the actual data patterns rather than merely following them, which suggests a possibility for greater accuracy.

-Consistent Decline in Predictions:

Downtrend in 2023: Regardless of the model used, all forecasts uniformly point to a decline in trends during the second half of 2023. This might be explained by the historical data's apparent cyclical nature. In other words, the historical data reveals a pattern in which peaks frequently appear in the middle of the year and then descend into troughs at year's end and the beginning of the next year. This apparent phenomena may have a significant impact on the expected downturn in 2023.

In light of the above, it is crucial to approach model selection with a nuanced comprehension of the underlying data and impacting factors, even though the ETS model shows a potentially higher prediction accuracy within the given circumstances. 

Therefore, I would like to conduct an accuracy analysis upon both arima and ets models.


# Accuracy assessment for ARIMA and ETS


```{r}
# Lists to store forecasts and error metrics
arima_forecast_list <- list()
ets_forecast_list <- list()
arima_error_metrics <- list()
ets_error_metrics <- list()
```


```{r}
# Forecasting with ARIMA and ETS
for (col in cols_to_forecast) {
  ts_data <- ts(training_data[[col]], start = c(start_year, start_month), frequency = 12)
  
  # ARIMA model
  arima_model <- auto.arima(ts_data)
  arima_forecast <- forecast(arima_model, h = h)
  arima_forecast_list[[col]] <- arima_forecast
  
  # ETS model
  tryCatch({
    ets_model <- ets(ts_data)
    ets_forecast <- forecast(ets_model, h = h)
    ets_forecast_list[[col]] <- ets_forecast
  }, error = function(e) {
    cat("Error in forecasting for column:", col, "with message:", e$message, "\n")
  })
}
```

## Calculating Accuracy Metrics for ARIMA and ETS
```{r}

for (col in cols_to_forecast) {
  actual <- test_data[[col]]
  
  arima_forecasted <- head(arima_forecast_list[[col]]$mean, validation_period)
  
  # ARIMA metrics
  arima_MAE <- mae(actual, arima_forecasted)
  arima_RMSE <- rmse(actual, arima_forecasted)
  arima_MAPE <- tryCatch(mape(actual, arima_forecasted), error = function(e) NA)
  arima_error_metrics[[col]] <- list(MAE = arima_MAE, RMSE = arima_RMSE, MAPE = arima_MAPE)
  
  # Check if ETS forecast exists for the column to avoid errors
  if (!is.null(ets_forecast_list[[col]])) {
    ets_forecasted <- head(ets_forecast_list[[col]]$mean, validation_period)
    
    # ETS metrics
    ets_MAE <- mae(actual, ets_forecasted)
    ets_RMSE <- rmse(actual, ets_forecasted)
    ets_MAPE <- tryCatch(mape(actual, ets_forecasted), error = function(e) NA)
    ets_error_metrics[[col]] <- list(MAE = ets_MAE, RMSE = ets_RMSE, MAPE = ets_MAPE)
  }
}

```



### Displaying Accuracy Metrics for each Column
```{r}
for (col in cols_to_forecast) {
  cat("\nAccuracy metrics for:", col, "\n")
  cat("--------------------------\n")
  cat("ARIMA:\n")
  print(arima_error_metrics[[col]])
  cat("ETS:\n")
  print(ets_error_metrics[[col]])
  cat("\n")
  cat("----------------------------------------------------\n")
}

```


- With Cardiff & Vale being a significant exception, most places displayed reduced error metrics when modeled with ETS, signifying a higher predictive accuracy as compared to the ARIMA model.

- Model Differentiation: 

ARIMA, A Time-Series Tradition Autoregressive, integrative (difference), and moving average features are included in the forecasting core components.
The foundation is laid by differencing to make data stationary, and then autoregressive and moving average techniques are applied.

ETS: A Comprehensive Method for Time-Series Model Constituent Elements reflects the seasonality, trend, and error components in time series data. Consists of rigorous detection and modeling of error, trend, and seasonal patterns within the data, which tends to include entire pattern analysis.


- According to the interest expressed by the NHS executives, I would like to provide some insights for the error metrics according to the quality of the forecast for the non-zero count data:


1. **Root Mean Squared Error (RMSE)**
   - **Description**: Measures the square root of the average squared differences between forecast and actual values.
   - **Pros**: It gives more weight to large errors.
   - **Cons**: It can be influenced significantly by outliers. RMSE might be inflated, if the count data is prone to significant spikes or declines, which should not be the problem for this data except for the Covid-19 era.
   
2. **Mean Absolute Percentage Error (MAPE)**
   - **Description**: Measures the average of the absolute percentage errors.
   - **Pros**: It is a relative metric, and is scale-independent and easy to interpret.
   - **Cons**: MAPE can overemphasize relative errors on small counts.
   
3. **Mean Absolute Error (MAE)**
   - **Description**: Measures the average of the absolute differences between forecast and actual values.
   - **Pros**: It's less sensitive to outliers than RMSE. It provides a straightforward average error size.
   - **Cons**: It does not emphasize large errors.

**However, RMSE and MAE should not be used for the hierarchical time series data.**
And here is why:

- **1. Consistency Between Levels:**
Granularity Errors are not consistently distributed or similar across all hierarchical levels due to the variance in data granularity between levels, and there will be an aggregation dilemma. Due to the variation in magnitude and volume at various hierarchical levels, direct aggregation of RMSE or MAE from bottom levels to top levels might produce inconsistent or biased judgments of forecast accuracy.

- **2. Problems with reconciliation:**
It is possible for forecasts to not add up coherently in a hierarchical structure when they are independently generated for several levels. Since they assess mistakes without taking into account hierarchical dependencies, RMSE and MAE do not by default address this reconciliation. And the alignment issue will therefore be caused. The use of standard error measurements at various hierarchical levels may produce false conclusions regarding the precision and dependability of the forecasts in the absence of a cogent reconciliation process.

- **3. Metric incomparability:**
Impact of a unit error is not constant across levels; for instance, a 100-unit error may be insignificant at a higher aggregate level but significant at a lower one. Additionally, due to the divergent scales of the errors at various levels, it is difficult to use a single statistic, such as RMSE or MAE, to fairly compare performance across all hierarchical levels.




# Reconciliation

## Aggregate data

```{r}
# hierarchy with age group
data_hts <- data_grouped_age |>
  aggregate_key(Aggregated_Organisation / Hospital_ItemName_ENG * Grouped_Age, attendance = sum(Data))
```



```{r}
# hierarchy without age group, only aggregate with the organisation, wales as a big picture
data_wales <- data_grouped_age |>
  aggregate_key(Aggregated_Organisation, attendance = sum(Data))
```




## Time series cross validation


```{r}
library(tscount)
# Split data
training_data <- data_wales %>% 
  filter(YearMonth <= yearmonth("2022 Jun"))

test_data <- data_wales %>% 
  filter(YearMonth > yearmonth("2022 Jun")) 

test_data_filtered <- test_data %>%
  filter(Aggregated_Organisation == "<aggregated>")

```


```{r}
# average the three models
library(fable.tscount)
fit_cv<- training_data %>%
  model(
    naive_model = NAIVE(attendance),
    ets_model = ETS(attendance),
    tscount = TSCOUNT(attendance ~ trend() , link = "log", model = list(past_obs = 1:3))
  )|> mutate (comb = (naive_model+ets_model+tscount)/3)
```




```{r}
# Generate forecasts
fc_cv <- fit_cv %>%
  forecast(h = 12) %>%
  filter(.model == "comb") 

```


```{r}
# Generate forecasts for aggregated
fc_cv_filtered <- fit_cv %>%
  forecast(h = 12) %>%
  filter(.model == "comb") %>%
  filter(Aggregated_Organisation == "<aggregated>")
```


```{r}
# Check the accuracy
forecasted_values <- fc_cv_filtered$.mean

accuracy_value <- accuracy(
  as.data.frame(forecasted_values),as.data.frame(test_data_filtered$attendance))

print(accuracy_value)


```


```{r}

joined_data <- as_tibble(test_data) %>%
  left_join(as_tibble(fc_cv), by = c("YearMonth", "Aggregated_Organisation")) %>%
  select(-attendance.y) %>%
  rename(attendance = attendance.x) %>%
  rename(forecast = .mean)

```


```{r result_not_expected}

comparison_data <- data.frame(
  Month = yearmonth(test_data$YearMonth),
  Actual = test_data$attendance,
  Forecasted = fc_cv$.mean,
  Aggregated_Organisation = test_data$Aggregated_Organisation  
) %>%rowwise()

```


```{r}

joined_data_long<- joined_data %>%
  pivot_longer(
    cols = c(attendance, forecast),
    names_to = "Type",
    values_to = "Value"
  )



unique_organisations <- unique(joined_data_long$Aggregated_Organisation)

plot_list <- map(unique_organisations, ~{
  ggplot(joined_data_long[joined_data_long$Aggregated_Organisation == .x,], 
         aes(x = YearMonth, y = Value, color = Type)) +
    geom_line() +
    labs(title = paste("Forecasted vs Actual Values for", .x),
         y = "Values") +
    theme_minimal()
})
print(plot_list)


```

```{r}
# Running t-tests and extracting p-value and t-value
t_test_summary <- joined_data %>%
  summarise(
    t_value = list(t.test(attendance, forecast)$statistic),
    p_value = list(t.test(attendance, forecast)$p.value)
  ) %>%
  unnest(cols = c(t_value, p_value))

print(t_test_summary)

```
The p value is relatively high. It suggests that we fail to reject the null hypothesis, meaning there is no enough evidence to conclude that forecast and actual values are significant different. In other words, the t-value of 0.313 is not large enough to be considered statistically significant.




- Base forecasting: applied when forecasting simple time series.
- Reconciliation forecasting (Hierarchical forecasting): it includes forecasting on different levels of aggregation, and it should be applied when dealing with hierarchical or grouped time series.


## Reconcile

### 1. Fitting models and Forecasting



```{r}
fit_wales <- data_wales |>
  model(
    naive_model = NAIVE(attendance),
    ets_model = ETS(attendance),
    tscount = TSCOUNT(attendance ~ trend() , link = "log", model = list(past_obs = 1:3))
  )|> mutate (comb = (naive_model+ets_model+tscount)/3)
```
  
```{r}
wales_rc <- fit_wales %>%
   reconcile(bu = bottom_up(comb),
            wls_ETS1 = min_trace(comb, method = "wls_struct"),
            wls_ETS2 = min_trace(comb, method = "wls_var"),
            mint_ETS = min_trace(comb, method = "mint_shrink")) %>%
  forecast(h = 6)
```




```{r}
forecast_wales_avg <- wales_rc %>%
  as_tibble() %>%
  group_by(Aggregated_Organisation, YearMonth) %>%
  summarise(mean_attendance = mean(.mean, na.rm = TRUE), .groups = "drop")

```

```{r}
forecast_wales_avg$Aggregated_Organisation <- factor(forecast_wales_avg$Aggregated_Organisation)

```

```{r}
ggplot(forecast_wales_avg, aes(x = YearMonth, y = mean_attendance)) +
  geom_line() +
  labs(title = "Average Attendance Forecast",
       x = "Year and Month",
       y = "Average Attendance") +
  facet_wrap(~Aggregated_Organisation, scales = "free_y") +
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 0, hjust = 1))
  
```




## Include Age Group



```{r}
data_full <- data_grouped_age |>
  aggregate_key((Aggregated_Organisation/Hospital_ItemName_ENG) * Grouped_Age, attendance = sum(Data))

filter_date <- yearmonth("2022 Jun")

fc <- data_full %>%
  filter(yearmonth(YearMonth) <= filter_date) %>%
  model(
    base = ETS(attendance)
  ) %>%
  reconcile(
    bu = bottom_up(base)) %>%
  forecast(h = 18)

```



```{r}

# Filter the forecast results for aggregated levels and plot
fc %>%
  filter(is_aggregated(Aggregated_Organisation), !is_aggregated(Grouped_Age)) %>%
  autoplot(
    data_full %>% filter(yearmonth(YearMonth) >= yearmonth("2020 Jan")),level = NULL
  ) +
  labs(y = "Attendance ('000)") +
  facet_wrap(vars(Grouped_Age), scales = "free_y") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.text.y = element_text(angle = 0, hjust = 1))

```



